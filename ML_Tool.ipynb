{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22876b2",
   "metadata": {
    "heading_collapsed": true,
    "hide_input": true
   },
   "source": [
    "# ML TOOL \n",
    "\n",
    "<span> A jupyter notebook tool to aid the testing of machine learning (ML) models in the domain of activity detection.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d52b7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Source: https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\n",
    "<div></div>\n",
    "Source: https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html\n",
    "<div></div>\n",
    "<font color='green'>Run the cell below to enable minimization of code blocks for better looking UI.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4163fb",
   "metadata": {
    "hidden": true,
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    function luc21893_refresh_cell(cell) {\n",
       "        if( cell.luc21893 ) return;\n",
       "        cell.luc21893 = true;\n",
       "        console.debug('New code cell found...' );\n",
       "        \n",
       "        var div = document.createElement('DIV');            \n",
       "        cell.parentNode.insertBefore( div, cell.nextSibling );\n",
       "        div.style.textAlign = 'right';\n",
       "        var a = document.createElement('A');\n",
       "        div.appendChild(a);\n",
       "        a.href='#'\n",
       "        a.luc21893 = cell;\n",
       "        a.setAttribute( 'onclick', \"luc21893_toggle(this); return false;\" );\n",
       "\n",
       "        cell.style.visibility='hidden';\n",
       "        cell.style.position='absolute';\n",
       "        a.innerHTML = '[show code]';        \n",
       "                \n",
       "    }\n",
       "    function luc21893_refresh() {                \n",
       "        if( document.querySelector('.code_cell .input') == null ) {            \n",
       "            // it apeears that I am in a exported html\n",
       "            // hide this code\n",
       "            var codeCells = document.querySelectorAll('.jp-InputArea')\n",
       "            codeCells[0].style.visibility = 'hidden';\n",
       "            codeCells[0].style.position = 'absolute';                        \n",
       "            for( var i = 1; i < codeCells.length; i++ ) {\n",
       "                luc21893_refresh_cell(codeCells[i].parentNode)\n",
       "            }\n",
       "            window.onload = luc21893_refresh;\n",
       "        }                 \n",
       "        else {\n",
       "            // it apperas that I am in a jupyter editor\n",
       "            var codeCells = document.querySelectorAll('.code_cell .input')\n",
       "            for( var i = 0; i < codeCells.length; i++ ) {\n",
       "                luc21893_refresh_cell(codeCells[i])\n",
       "            }            \n",
       "            window.setTimeout( luc21893_refresh, 1000 )\n",
       "        }        \n",
       "    }\n",
       "    \n",
       "    function luc21893_toggle(a) {\n",
       "        if( a.luc21893.style.visibility=='hidden' ) {\n",
       "            a.luc21893.style.visibility='visible';        \n",
       "            a.luc21893.style.position='';\n",
       "            a.innerHTML = '[hide code]';\n",
       "        }\n",
       "        else {\n",
       "            a.luc21893.style.visibility='hidden';        \n",
       "            a.luc21893.style.position='absolute';\n",
       "            a.innerHTML = '[show code]';\n",
       "        }\n",
       "    }\n",
       "    \n",
       "    luc21893_refresh()\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML \n",
    "<script>\n",
    "    function luc21893_refresh_cell(cell) {\n",
    "        if( cell.luc21893 ) return;\n",
    "        cell.luc21893 = true;\n",
    "        console.debug('New code cell found...' );\n",
    "        \n",
    "        var div = document.createElement('DIV');            \n",
    "        cell.parentNode.insertBefore( div, cell.nextSibling );\n",
    "        div.style.textAlign = 'right';\n",
    "        var a = document.createElement('A');\n",
    "        div.appendChild(a);\n",
    "        a.href='#'\n",
    "        a.luc21893 = cell;\n",
    "        a.setAttribute( 'onclick', \"luc21893_toggle(this); return false;\" );\n",
    "\n",
    "        cell.style.visibility='hidden';\n",
    "        cell.style.position='absolute';\n",
    "        a.innerHTML = '[show code]';        \n",
    "                \n",
    "    }\n",
    "    function luc21893_refresh() {                \n",
    "        if( document.querySelector('.code_cell .input') == null ) {            \n",
    "            // it apeears that I am in a exported html\n",
    "            // hide this code\n",
    "            var codeCells = document.querySelectorAll('.jp-InputArea')\n",
    "            codeCells[0].style.visibility = 'hidden';\n",
    "            codeCells[0].style.position = 'absolute';                        \n",
    "            for( var i = 1; i < codeCells.length; i++ ) {\n",
    "                luc21893_refresh_cell(codeCells[i].parentNode)\n",
    "            }\n",
    "            window.onload = luc21893_refresh;\n",
    "        }                 \n",
    "        else {\n",
    "            // it apperas that I am in a jupyter editor\n",
    "            var codeCells = document.querySelectorAll('.code_cell .input')\n",
    "            for( var i = 0; i < codeCells.length; i++ ) {\n",
    "                luc21893_refresh_cell(codeCells[i])\n",
    "            }            \n",
    "            window.setTimeout( luc21893_refresh, 1000 )\n",
    "        }        \n",
    "    }\n",
    "    \n",
    "    function luc21893_toggle(a) {\n",
    "        if( a.luc21893.style.visibility=='hidden' ) {\n",
    "            a.luc21893.style.visibility='visible';        \n",
    "            a.luc21893.style.position='';\n",
    "            a.innerHTML = '[hide code]';\n",
    "        }\n",
    "        else {\n",
    "            a.luc21893.style.visibility='hidden';        \n",
    "            a.luc21893.style.position='absolute';\n",
    "            a.innerHTML = '[show code]';\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    luc21893_refresh()\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6a9ea",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Dependencies installation\n",
    "\n",
    "<font color='green'>Run this cell to install all required libraries.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cb734-9a3e-4927-951b-34f48710f156",
   "metadata": {
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/ehl/torch_stable.html\n",
    "%pip install timm==0.4.12\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "%pip install ipywidgets==7.4.2\n",
    "%pip install tqdm\n",
    "%pip install pandas\n",
    "%pip install moviepy\n",
    "%pip install opencv-python\n",
    "%pip install jupyter_contrib_nbextensions\n",
    "%pip install opencv-python\n",
    "'''\n",
    "if you are having issues installing moviepy, do \n",
    "%pip install --upgrade pip\n",
    "then \n",
    "%pip install moviepy\n",
    "'''\n",
    "\n",
    "# Installation of required Libraries \n",
    "!pip install ipywidgets\n",
    "!pip install jupyter-js-widgets-nbextension\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "!pip install ipyfilechooser\n",
    "!pip install wandb\n",
    "!pip install wandb -qU\n",
    "!jupyter contrib nbextension install --user\n",
    "\n",
    "print(\"All dependency installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fadfc0-0020-455b-b1e6-910a8b72209c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Import all Dependencies and Setup\n",
    "<font color='green'>Run this cell to import all required libraries.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d1a5b-c24b-4b2a-9081-4f0a1c504a8f",
   "metadata": {
    "hidden": true,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import wandb\n",
    "import itertools  \n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from subprocess import Popen\n",
    "from datetime import timedelta\n",
    "from ipyfilechooser import FileChooser\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import Video\n",
    "\n",
    "print(\"Running on touch version \" + torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094899c0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e892b",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### R1: An interactive python Jupyter notebook in a github repository\n",
    "- Ensure jupyter notebook has python3.x \n",
    "- Locally interact with the notebook on a local linux, mac or windows environment that has python 3.x, Jupyter and other necessary dependencies installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a1b8c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-01\n",
    "<font color='green'>Run this cell to ensure notebook is running on python 3.8.x.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d9fb6-dec4-4ed3-8fa2-2c2cfbd5ff01",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(\"Running on Python Version \" + python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247b960",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### R2.: Data Exploration Section\n",
    "- A data folder in the repo with subfolders that store input video files and other related input files for different types of datasets\n",
    "- Choose a video file from the data folder through an appropriate UI component (e.g., dropdown menu) in a notebook code cell\n",
    "- See video playback of the chosen video file in an output cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a418e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-03\n",
    "<font color='green'>Run this cell to create to ensure `./data` folder is created.</font>\n",
    "<br>\n",
    "<font color='green'>Sub-folder `./data/RGB_videos` folder is created to store **Toyota Smarthome dataset**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b1973",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create data folder if it does not exist \n",
    "data_directory = Path('./data')\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "    \n",
    "# Create RGB video subfoler \n",
    "directory = data_directory/\"RGB_videos\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(\"Created \" + str(directory))\n",
    "else:\n",
    "    print(\"Sub-folder \" +str(directory) + \" exist\")\n",
    "    \n",
    "# Check if videos exist in subfolder \n",
    "if os.path.isdir(directory) and os.path.exists(directory):\n",
    "    if len(os.listdir(directory)) == 0:\n",
    "        print(\" \")\n",
    "        print(str(directory) + \" is empty\")\n",
    "        print(\"Please upload video data to this file path: \"+ str(directory))\n",
    "    else:    \n",
    "        print(\" \")\n",
    "        print(str(directory) + \" is not empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256964f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### US-04 & US-05\n",
    "<font color='green'>Select Video to playback.</font>\n",
    "<div></div>\n",
    "<font color='green'>Run this cell to select video input for data exploration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578ab5c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select a folder from the datasets folder created\n",
    "starting_directory = data_directory\n",
    "folderupload = FileChooser(starting_directory)\n",
    "# Display only MP4 files\n",
    "folderupload.filter_pattern = ['*.mp4', '*.flv']\n",
    "folderupload.title = '<b>Select Dataset to playback<b>'\n",
    "display(folderupload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f820ac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Run this cell to play selected video.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3605cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Play the video from selected local path using video()\n",
    "Video(folderupload.selected, embed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce27c1",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# TSU Pipeline\n",
    "\n",
    "<font color='grey'>This section is for inference, training and testing using the TSU pipeline.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517176f9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Feature Extraction \n",
    "\n",
    "<font color='grey'>Extract video features from raw videos using multiple GPUs using v-iashin repository. We will be using RAFT flow frames as well as I3D features.</font>\n",
    "\n",
    "Source: https://github.com/v-iashin/video_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9869dd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prepare for Extraction\n",
    "- Clone the v-iashin repository\n",
    "- Create required folders to store the RGB, RGB+FLOW and FLOW extracted files\n",
    "- Check cuda device available and extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea426f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(\"------------------------------- SUMMARY OF FOLDER CREATION -------------------------------------------\")\n",
    "# Create data folder if it does not exist \n",
    "RGB_directory = Path('./data/RGB_i3d_16frames_extracted/')\n",
    "if not os.path.exists(RGB_directory):\n",
    "    os.makedirs(RGB_directory)\n",
    "    print(\"Created \" + str(RGB_directory))\n",
    "else:\n",
    "    print(str(RGB_directory) + \" exist\") \n",
    "    \n",
    "# Create data folder if it does not exist \n",
    "FLOW_directory = Path('./data/FLOW_i3d_16frames_extracted/')\n",
    "if not os.path.exists(FLOW_directory):\n",
    "    os.makedirs(FLOW_directory)\n",
    "    print(\"Created \" + str(FLOW_directory))\n",
    "else:\n",
    "    print(str(FLOW_directory) + \" exist\") \n",
    "    \n",
    "# Create data folder if it does not exist \n",
    "FLOWRGB_directory = Path('./data/FLOWnRGB_i3d_16frames_extracted/')\n",
    "if not os.path.exists(FLOWRGB_directory):\n",
    "    os.makedirs(FLOWRGB_directory)\n",
    "    print(\"Created \" + str(FLOWRGB_directory) +\"\\n\\n\")\n",
    "else:\n",
    "    print(str(FLOWRGB_directory) + \" exist\\n\\n\") \n",
    "    \n",
    "repo = Path('./video_features/')\n",
    "# Clone the v-iashin repo\n",
    "if not os.path.exists(repo):\n",
    "    ! git clone https://github.com/v-iashin/video_features.git\n",
    "        \n",
    "! pip install omegaconf==2.0.6\n",
    "\n",
    "%cd ./video_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17fe37",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-06 \n",
    "<font color='green'>**Extract** features **using I3D feature** folder to RGB, FLOW and RGB+FLOW **NPY files** (**GPU only**).  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910dd9c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dont run this cell if u no CUDA!!!\n",
    "from models.i3d.extract_i3d import ExtractI3D\n",
    "from utils.utils import build_cfg_path\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device is \" + device)\n",
    "print(\"CUDA device name:\")\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929844e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '../data/RGB_videos/'\n",
    "\n",
    "onlyfiles = [mypath+f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(\"Raw video file that are going to be extracted:\\n\" + str(onlyfiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62a66f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### RGB \n",
    "<font color='green'>**Extract** files are stored in `./data/RGB_i3d_16frames_extracted/` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18692d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import os\n",
    "\n",
    "# Select the feature type\n",
    "feature_type = 'i3d'\n",
    "\n",
    "# Load and patch the config\n",
    "args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "args.video_paths = onlyfiles\n",
    "# args.show_pred = True\n",
    "# args.stack_size = 24\n",
    "# args.step_size = 24\n",
    "args.extraction_fps = 16\n",
    "args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
    "args.streams = 'rgb'\n",
    "#args.on_extraction = 'save_numpy'\n",
    "#args.output_path = './sample/Output'\n",
    "\n",
    "\n",
    "# Load the model\n",
    "extractor = ExtractI3D(args)\n",
    "\n",
    "# Extract features\n",
    "for video_path in args.video_paths:\n",
    "    print(f'Extracting for {video_path}')\n",
    "    base=os.path.basename(video_path)\n",
    "    fileName = os.path.splitext(base)[0]\n",
    "    \n",
    "    feature_dict = extractor.extract(video_path)\n",
    "    #[(print(k), print(v.shape), print(v)) for k, v in feature_dict.items()]\n",
    "    extracted_data = list(feature_dict.values())[0]\n",
    "    #print(extracted_data)\n",
    "    save(\"../data/RGB_i3d_16frames_extracted/\"+fileName+\".npy\",extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a32f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### FLOW \n",
    "<font color='green'>**Extract** files are stored in `./data/FLOW_i3d_16frames_extracted/` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ae89b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import os\n",
    "\n",
    "# Select the feature type\n",
    "feature_type = 'i3d'\n",
    "\n",
    "# Load and patch the config\n",
    "args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "args.video_paths = onlyfiles\n",
    "# args.show_pred = True\n",
    "# args.stack_size = 24\n",
    "# args.step_size = 24\n",
    "args.extraction_fps = 16\n",
    "args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
    "args.streams = 'flow'\n",
    "#args.on_extraction = 'save_numpy'\n",
    "#args.output_path = './sample/Output'\n",
    "\n",
    "\n",
    "# Load the model\n",
    "extractor = ExtractI3D(args)\n",
    "\n",
    "# Extract features\n",
    "for video_path in args.video_paths:\n",
    "    print(f'Extracting for {video_path}')\n",
    "    base=os.path.basename(video_path)\n",
    "    fileName = os.path.splitext(base)[0]\n",
    "    \n",
    "    feature_dict = extractor.extract(video_path)\n",
    "    #[(print(k), print(v.shape), print(v)) for k, v in feature_dict.items()]\n",
    "    extracted_data = list(feature_dict.values())[0]\n",
    "    #print(extracted_data)\n",
    "    save(\"../data/FLOW_i3d_16frames_extracted/\"+fileName+\".npy\",extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8131f12",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### FLOW + RGB\n",
    "<font color='green'>**Extract** files are stored in `./data/FLOWnRGB_i3d_16frames_extracted/` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1bbdd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "import os\n",
    "\n",
    "# Select the feature type\n",
    "feature_type = 'i3d'\n",
    "\n",
    "# Load and patch the config\n",
    "args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "args.video_paths = onlyfiles\n",
    "# args.show_pred = True\n",
    "# args.stack_size = 24\n",
    "# args.step_size = 24\n",
    "args.extraction_fps = 16\n",
    "args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
    "#args.streams = 'rgb'\n",
    "#args.on_extraction = 'save_numpy'\n",
    "#args.output_path = './sample/Output'\n",
    "\n",
    "\n",
    "# Load the model\n",
    "extractor = ExtractI3D(args)\n",
    "\n",
    "\n",
    "# Extract features\n",
    "for video_path in args.video_paths:\n",
    "    print(f'Extracting for {video_path}')\n",
    "    base=os.path.basename(video_path)\n",
    "    fileName = os.path.splitext(base)[0]\n",
    "    \n",
    "    feature_dict = extractor.extract(video_path)\n",
    "    #[(print(k), print(v.shape), print(v)) for k, v in feature_dict.items()]\n",
    "    extracted_rgb = list(feature_dict.values())[0]\n",
    "    extracted_flow = list(feature_dict.values())[1]\n",
    "    comb = np.column_stack((extracted_rgb, extracted_flow))\n",
    "    np.save(\"../data/FLOWnRGB_i3d_16frames_extracted/\"+fileName+\".npy\",comb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc59d34",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Return to main folder...\")\n",
    "#if \n",
    "#%cd ./T01-nvidia-jupyternotebookenv\n",
    "\n",
    "# get current directory\n",
    "path = os.getcwd()\n",
    "base=os.path.basename(path)\n",
    "fileName = os.path.splitext(base)[0]\n",
    "#print(fileName)\n",
    "\n",
    "if fileName == \"video_features\":\n",
    "    %cd ..\n",
    "    \n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b3798",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inference Section for TSU pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c27062",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### R3: Perform inference using a pretrained model based on the TSU project\n",
    "- Load a pretrained model using an appropriate UI component\n",
    "- Choose an input video, using an appropriate UI component, from the TSU project\n",
    "- See inference results in the form of an output video with captions that indicate the current detected activity in each video frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2454f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-06 \n",
    "<font color='green'>**Load** a **pretrained model** and extracted **feature data** folder to perform inference. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0994a3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set widget boxes for each variable we are loading, in this case, a pretrained model and a feature data.\n",
    "container_1 = widgets.Box()\n",
    "container_2 = widgets.Box()\n",
    "\n",
    "# open trained_model directory and lsit trained models there\n",
    "# selectedModel.value will be the trained model name\n",
    "trained_model_dir = './trained_model/'\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = trained_model_dir\n",
    "selectedModel = FileChooser(starting_directory)\n",
    "selectedModel.title = '<b>Select a pretrained model to perform inference <b>'\n",
    "feature_data = './data/'\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = feature_data\n",
    "selectedFeatureData = FileChooser(starting_directory)\n",
    "selectedFeatureData.show_only_dirs = True\n",
    "selectedFeatureData.title = '<b>Select a feature data folder to perform inference <b>'\n",
    "\n",
    "# Set variables to get selected iputs from user\n",
    "control_1 = selectedModel\n",
    "control_2 = selectedFeatureData\n",
    "container_1.children = [control_1]\n",
    "container_2.children = [control_2]\n",
    "\n",
    "# Load UI for user to select model and feature data\n",
    "inputtabs = widgets.Tab()\n",
    "inputtabs.children = [container_1, container_2]\n",
    "inputtabs.set_title(0, \"Pre-trained Model\")\n",
    "inputtabs.set_title(1, \"Feature Data\")\n",
    "inputtabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ab53d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>See current configuration and confirm.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a581d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Function to let user know what to do next after confirming configuration\n",
    "def inference(b):\n",
    "    print(\"Run next cell to start inferencing.\")\n",
    "\n",
    "# Set some variables to extract user selection to display user's current configuration\n",
    "selectedFeatureDataPath = str(selectedFeatureData.selected_path).split('/')\n",
    "selectedModel = selectedModel.selected.split('/')\n",
    "selectedModel = selectedModel[len(selectedModel)-1]\n",
    "selectedFeature = selectedFeatureDataPath[len(selectedFeatureDataPath)-1]\n",
    "\n",
    "# Output user's configuration so that user has clearer idea of what they have selected\n",
    "print(\"------------------------------ SUMMARY OF INFERENCE RUN CONFIGURATION ------------------------------\")\n",
    "print(\"Running inference with...\")\n",
    "print(\"Model: \" + str(selectedModel))\n",
    "print(\"Feature Data: \" + selectedFeature)\n",
    "\n",
    "# Button for user to confirm their confirmation\n",
    "button = widgets.Button(\n",
    "    description='Confirm Config',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Confirm Configuration',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Display button and allow user to confirm configuration\n",
    "button.on_click(inference)\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c4aac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Initiate inference with TSU's pipeline and current configuration. (No GPU to run locally, all outputs are taken from our colab's iterations)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350c494",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!chmod 755 -R ./Toyota_Smarthome/pipline/run_PDAN.sh\n",
    "# Run using Popen and output to appropriate files for diaply later\n",
    "with open(\"./temp/stdout.txt\",\"wb\") as out, open(\"./temp/stderr.txt\",\"wb\") as err:\n",
    "    subprocess.Popen(\"./pipline/run_PDAN.sh %s %s\" % (str(selectedFeatureData.selected_path), str(selectedModel.selected)),stdout=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340cd9f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Extract key values from inference result for each video, map them to the appropriate actions and output them to a csv for captions injection.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb57a47",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Output array to read output from popen function\n",
    "output = []\n",
    "\n",
    "# set path variables to open and read line from files\n",
    "resultOutputPath = \"./Annotation/results/\"\n",
    "csvOutputName = resultOutputPath\n",
    "\n",
    "# Read the the key to action mapping file\n",
    "actionMapDataFrame = pd.read_csv(\"./Annotation/labels.csv\")\n",
    "print(\"Reading labels from ./Annotation/labels.csv\")\n",
    "\n",
    "# Creation of dataframe for inferred captions\n",
    "captionsByFrame = {'captions' : []}\n",
    "#print(actionMapDataFrame[\"Event\"][0]) get 0 key action\n",
    "# open inference results, input to array\n",
    "print(\"Reading inference reults from ./temp/stdout.txt\")\n",
    "\n",
    "# open file and read line each to build output array data structure\n",
    "with open (\"./temp/stdout.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        output.append(line)\n",
    "   \n",
    "# map each action element to its label, multiply by 16frames and output into csv\n",
    "for line in output:\n",
    "    if \"video\" in line:\n",
    "        csvOutputName = resultOutputPath\n",
    "        csvOutputName += line.split(\":\")[1].replace('\\n',\"\").replace(\" \",\"\") + \".csv\"\n",
    "        # reset captions for new video\n",
    "        captionsByFrame = {'captions' : []}\n",
    "        \n",
    "    else:\n",
    "        #actions:  [ 0  0  0 ... 26 26 26] end\n",
    "        if \"actions\" in line and \"end\" in line:\n",
    "            actions = line.replace(\"actions: \",\"\").replace(\"end\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\"...\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            \n",
    "            for action in actions:\n",
    "                actionKey = int(action)\n",
    "                for frame in range(16):\n",
    "                     captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "\n",
    "            captionsDF = pd.DataFrame(captionsByFrame)\n",
    "            captionsDF.to_csv(csvOutputName) \n",
    "\n",
    "        elif \"actions\" in line:\n",
    "            #actions:  [ 1  0  0  0  0  0  0  0  1  1  1  1  1  1  1 16  1  1 22 22 21 22 22 22\n",
    "            actions = line.replace(\"actions: \",\"\").replace(\"[\",\"\").replace(\"\\n\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            for action in actions:\n",
    "                actionKey = int(action)\n",
    "                for frame in range(16):\n",
    "                     captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "                        \n",
    "        elif \"evaluation\" not in line and \"rgb\" not in line and \"end\" not in line:\n",
    "            if line == \"1\\n\" or line == \"2\\n\":\n",
    "                continue\n",
    "            actions = line.replace(\"\\n\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            for action in actions:\n",
    "                try:\n",
    "                    actionKey = int(action)\n",
    "\n",
    "                    for frame in range(16):\n",
    "                         captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "                except:\n",
    "                    pass\n",
    "        elif \"end\" in line:\n",
    "            actions = line.replace(\"]\",\"\").replace(\"end\\n\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            for action in actions:\n",
    "\n",
    "                actionKey = int(action)\n",
    "\n",
    "                for frame in range(16):\n",
    "                    captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "\n",
    "            captionsDF = pd.DataFrame(captionsByFrame)\n",
    "            captionsDF.to_csv(csvOutputName)   \n",
    "            \n",
    "print(\" Inferred captions successfully mapped to actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f761b35",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-07\n",
    "Create annotations file if it does not exist. </font>\n",
    "<br><br>\n",
    "<font color='green'>Select an input video from the TSU project to output video with captions based on annotations. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39bb93",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set paths for reading and user selection of captioned videos\n",
    "videoDir = 'data/RGB_videos/'\n",
    "inferredList = 'Annotation/results/'\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = videoDir\n",
    "selectedVideo = FileChooser(starting_directory)\n",
    "\n",
    "# Display only MP4 files\n",
    "selectedVideo.filter_pattern = ['*.mp4', '*.flv']\n",
    "selectedVideo.title = '<b>Select input video to output video with captions<b>'\n",
    "display(selectedVideo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610967fb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>View selected video.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f3698",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract user selcted video and store in a variable\n",
    "videoSelectedPath = selectedVideo.selected\n",
    "annotationsDir = 'Annotation/' + selectedVideo.selected_filename[0:3] + '/' + selectedVideo.selected_filename.split('.')[0] + '.csv'\n",
    "inferredDir = inferredList + selectedVideo.selected_filename.split('.')[0] + '.csv'\n",
    "\n",
    "# print user selection and configuration\n",
    "print(\"--------------------------------- SUMMARY OF SELECTED INFERNCE ---------------------------------\")\n",
    "#print(\"Ground Truth: \" + annotationsDir)\n",
    "#print(\"Inferred: \" + inferredDir)\n",
    "#print(\"\")\n",
    "print(f\"You have selected to view inference results for video '{selectedVideo.selected_filename}'. You will see a video comparing ground truth captions and inferred captions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eff59a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Inject captions into each frame of the video and output captioned video for display.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934d38f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pipeline function to insert text into each prame\n",
    "def pipeline(frame):\n",
    "    try:\n",
    "        cv2.rectangle(frame, (50, 380), (250, 430), (0, 0, 0), -1)\n",
    "        cv2.rectangle(frame, (350, 380), (500, 430), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, str(\"ground truth\"), (50,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, str(next(captionsDF)[1].captions), (50,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, str(\"inferred results\"), (350,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1) \n",
    "        try:\n",
    "            cv2.putText(frame, str(next(inferredCaptions)[1].captions), (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        except:\n",
    "            cv2.putText(frame, str(inferredDF['captions'][len(inferredDF)-1]), (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    # additional frame manipulation\n",
    "    return frame\n",
    "\n",
    "#read ground truth data\n",
    "df = pd.read_csv(annotationsDir)\n",
    "captionsByFrame = {'captions' : []}\n",
    "caption = \"\"\n",
    "totalEvents = len(df.index)\n",
    "oldStartFrame = 0\n",
    "\n",
    "#read inferred data\n",
    "inferredDF = pd.read_csv(inferredDir)\n",
    "\n",
    "#build new df\n",
    "for event in range(totalEvents):\n",
    "    #i need to minus old one\n",
    "    initialStartframe = int(df['start_frame'][event]) - oldStartFrame\n",
    "    for n in range(initialStartframe):\n",
    "        captionsByFrame['captions'].append(caption)\n",
    "    caption = str(df['event'][event])\n",
    "    oldStartFrame += initialStartframe\n",
    "    if (event == totalEvents-1):\n",
    "        initialStartframe = int(df['end_frame'][event]) - oldStartFrame\n",
    "        for n in range(initialStartframe):\n",
    "            captionsByFrame['captions'].append(caption)\n",
    "\n",
    "    \n",
    "captionsDF = pd.DataFrame(captionsByFrame).iterrows()\n",
    "inferredCaptions = inferredDF.iterrows()\n",
    "video = VideoFileClip(videoSelectedPath)\n",
    "out_video = video.fl_image(pipeline)\n",
    "\n",
    "# output path\n",
    "captionVideoPath = './video_withcaptions/' + selectedVideo.selected_filename\n",
    "out_video.write_videofile(captionVideoPath, audio=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196cb8c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### US-08\n",
    "<font color='green'>Play output video with captions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185def9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Video(captionVideoPath, embed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9465c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training Section for TSU pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86762d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### R4: Train a HOI ML model based on the TSU project.\n",
    "- Choose a dataset subfolder to use for the training\n",
    "- Initialize a model (to be trained) with a network architecture configured in a separate .py file\n",
    "- Specify a name for this new model\n",
    "- Set the batch_size and epochs\n",
    "- Run the training sequence\n",
    "- Show progress of the training in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104394a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### US-09, US-11 & US-12\n",
    "\n",
    "<font color='green'>1. Select a dataset subfolder from the data folder to use for training.</font>\n",
    "<br>\n",
    "<font color='green'>2. Specify a name for the new model.</font>\n",
    "<br>\n",
    "<font color='green'>3. Set the batch_size and epochs.</font>\n",
    "<br>\n",
    "<font color='green'>4. Check training configuration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dabcd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create model path if dont exist\n",
    "modelPath = Path('./trained_model')\n",
    "\n",
    "# Created output model dir if it does not exist\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "    #print(\"Created \" + str(modelPath))\n",
    "else:\n",
    "    #print(\"Folder \" + str(modelPath) + \" exist\")\n",
    "    pass\n",
    "    \n",
    "# if data path was not chosen previously\n",
    "if (str(selectedFeatureData.selected_path) == \"\"):\n",
    "    selectedFeatureData.selected_path = './data/RGB_i3d_16frames_64000_SSD'\n",
    "    \n",
    "# Create UI for user selection for training the model\n",
    "container_1 = widgets.VBox()\n",
    "container_2 = widgets.Box()\n",
    "container_3 = widgets.Box()\n",
    "\n",
    "out = widgets.Output()\n",
    "config_checks_output = widgets.Output()\n",
    "\n",
    "with out:\n",
    "    print(\"Current Selected Feature Data Path:\" + str(selectedFeatureData.selected_path) + \"\\nSelect a new folder if you wish to change it\" )\n",
    "\n",
    "starting_directory = data_directory\n",
    "dataset = FileChooser(starting_directory)\n",
    "# Switch to folder-only mode\n",
    "dataset.show_only_dirs = True\n",
    "dataset.title = '<b>Change Feature Data to use for training<b>'\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "modelPath = \"./trained_model/\"\n",
    "newModel = widgets.Text(\n",
    "    description='New Model Name:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "epoch = widgets.Text(\n",
    "    description='Epoch Value:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "batch_size = widgets.Text(\n",
    "    description='Batch Size:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "button_submit = widgets.Button(\n",
    "    description='Submit Configs',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Click to Submit Training Configurations'\n",
    ")\n",
    "\n",
    "control_1 = dataset\n",
    "control_5 = out\n",
    "control_2 = newModel\n",
    "control_3 = epoch\n",
    "control_4 = batch_size\n",
    "container_1.children = [control_5, control_1]\n",
    "container_2.children = [control_2]\n",
    "container_3.children = [control_3, control_4]\n",
    "\n",
    "accordions = widgets.Accordion()\n",
    "accordions.children = [container_1, container_2, container_3]\n",
    "accordions.set_title(0, \"Dataset\")\n",
    "accordions.set_title(1, \"Output Model\")\n",
    "accordions.set_title(2, \"Batch_size & Epochs\")\n",
    "\n",
    "def preview_training_config(newModel, epoch, batch_size):\n",
    "    print(\"\")\n",
    "    print(\"--------------------------------- SUMMARY OF TRAINING PARAMETERS ---------------------------------\")\n",
    "    print(\"Pipeline: TSU pipeline\")\n",
    "    if dataset.selected is not None:\n",
    "        selectedFeatureDataPath = str(dataset.selected_path)\n",
    "        selectedFeature = selectedFeatureDataPath.split('/')\n",
    "        selectedFeature = selectedFeature[len(selectedFeature)-1]\n",
    "        print(\"Feature Data: \" + selectedFeature)\n",
    "    else:\n",
    "        print(\"Feature Data: \")\n",
    "    print(\"Model name: \", newModel)\n",
    "    print(\"Epoch selected: \", epoch)\n",
    "    print(\"Batch size: \", batch_size)\n",
    "    \n",
    "def check_config_errors():\n",
    "    config_checks_output.clear_output()\n",
    "    with config_checks_output:\n",
    "        # Error checking \n",
    "        #if dataset.selected is not None:\n",
    "            #selectedFeatureDataPath = str(dataset.selected_path)\n",
    "\n",
    "        if not epoch.value.isdigit():\n",
    "            print(\"Epoch should be digit\")\n",
    "        if not batch_size.value.isdigit():\n",
    "            print(\"Batch size should be digit\")\n",
    "\n",
    "        modelPath = \"./trained_model/\"\n",
    "\n",
    "        # Display user configuration\n",
    "        modelPath += newModel.value\n",
    "        if not os.path.exists(modelPath):\n",
    "            print(\"Model \" + modelPath + \" has been initialized.\")\n",
    "            f = open(modelPath, 'w')\n",
    "            f.close()\n",
    "            print(\"Proceed to next step to train the model\")\n",
    "        else:\n",
    "            modelDir = \"./trained_model/\"\n",
    "            print(\"Model name exists or you did not specify a file name!\")     \n",
    "    \n",
    "        \n",
    "def submit_clicked(b):\n",
    "    check_config_errors()\n",
    "    \n",
    "#dynamically captures changes in input and returned new output\n",
    "training_configs = widgets.interactive_output(preview_training_config, {'newModel': newModel, 'epoch': epoch, 'batch_size': batch_size,})\n",
    "training_configs.layout = {'border': '1px solid black'}\n",
    "\n",
    "button_submit.on_click(submit_clicked)\n",
    "\n",
    "accordions = widgets.VBox([\n",
    "    accordions, \n",
    "    training_configs,\n",
    "    widgets.HBox([button_submit]),\n",
    "    config_checks_output\n",
    "])\n",
    "accordions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72c35e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Run next cell to run model training. (Run on a machine that has GPU)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e56b20",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!chmod 755 -R ./Toyota_Smarthome/pipline/run_PDAN.sh\n",
    "\n",
    "load_model = \"False\"\n",
    "if dataset.selected is not None:\n",
    "        selectedFeatureDataPath = str(dataset.selected_path)\n",
    "\n",
    "with open(\"./temp/stdout_training.txt\",\"wb\") as out, open(\"./temp/stderr.txt\",\"wb\") as err:\n",
    "    subprocess.Popen(\"./pipline/run_PDAN.sh %s %s %s %s %s %s\" % (str(selectedFeatureData.selected_path), str(load_model), str(\"True\"), str(epoch.value), str(batch_size.value), str(newModel.value)),stdout=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01df8d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-10, US-13, US-14 & US15\n",
    "\n",
    "<font color='green'>1. Separate .py file that configures the network architecture (**run_PDAN**).</font>\n",
    "<br>\n",
    "<font color='green'>2. Set configuration to run the training sequence.</font>\n",
    "<br>\n",
    "<font color='green'>3. Display progress of the training.</font>\n",
    "<br>\n",
    "<font color='green'>4. Save newly trained models in `./trained_model`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114dc1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#linux\n",
    "with open(\"./temp/stdout_training.txt\",\"wb\") as out:\n",
    "    subprocess.Popen(\"./pipline/run_PDAN.sh %s %s %s %s %s %s\" % (str(selectedFeatureDataPath), str(load_model), str(\"True\"), epoch.value, batch_size.value, str(modelPath)), stdout=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee4afd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output=[]\n",
    "# epoch element used for indexing epoch's training data i.e. epoch 2 loss\n",
    "epochArr = []\n",
    "trainMapArr = []\n",
    "trainLossArr = []\n",
    "valMapArr = []\n",
    "valLossArr = []\n",
    "\n",
    "with open (\"./temp/stdout_training.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        output.append(line)\n",
    "        \n",
    "for line in output:\n",
    "    if \"epoch\" in line:\n",
    "        epoch = line.replace(\"\\n\",\"\").replace(\"epoch: \",\"\").split(\" \")[1]\n",
    "        epochArr.append(int(epoch))\n",
    "    elif \"train-map\" in line:\n",
    "        trainMap = line.replace(\"train-map: \",\"\").replace(\"tensor(\",\"\").replace(\")\\n\",\"\")\n",
    "        trainMapArr.append(float(trainMap))\n",
    "    elif \"val_loss\" in line:\n",
    "        valLoss = line.replace(\"val_loss: \",\"\").replace(\"tensor(\",\"\").split(\",\")[0]\n",
    "        valLossArr.append(float(valLoss))\n",
    "    elif \"val-map\" in line:\n",
    "        valMap = line.replace(\"val-map: \",\"\").replace(\"tensor(\",\"\").replace(\")\\n\",\"\")\n",
    "        valMapArr.append(float(valMap))\n",
    "    elif \"train_loss\" in line:\n",
    "        trainLoss = line.replace(\"train_loss: \",\"\").replace(\"tensor(\",\"\").split(\",\")[0]\n",
    "        trainLossArr.append(float(trainLoss))\n",
    "        \n",
    "# whitney you can use these arrays to do visualisation for training   \n",
    "print(epochArr)\n",
    "print(trainMapArr)\n",
    "print(valLossArr)\n",
    "print(valMapArr)\n",
    "print(trainLossArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade1c15",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### TRAINING VISUALIZATION\n",
    "\n",
    "Source: https://docs.wandb.ai/guides/track/log/plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d712dd",
   "metadata": {},
   "source": [
    "#### US-29\n",
    "<font color='green'>Display the result of the training model:</font>\n",
    "1. Training Accuracy Precision\n",
    "2. Value Loss\n",
    "3. Value Accuracy Precision\n",
    "4. Training Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f751404",
   "metadata": {},
   "source": [
    "Source: https://docs.wandb.ai/quickstart\n",
    "\n",
    "<font color='green'>Login to wandb for evalutaion result evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6aa3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.login():\n",
    "    print(\"Currently logged in. Run next cell to see visualisation of training results.\")\n",
    "    print(\"If the visualisation is not displaying, request account from Whitney.\")\n",
    "else:\n",
    "    print(\"Not logged in to any account.\")\n",
    "    print(\"If API key is required, request from Whitney.\")\n",
    "\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6335ac1",
   "metadata": {},
   "source": [
    "<font color='green'>Visualize evalutated results.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e68ed",
   "metadata": {},
   "source": [
    "<font>1. Visualize Training Accuracy Precision results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f99f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training Accuracy Precision\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"3ocr5wb6\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f79fb",
   "metadata": {},
   "source": [
    "<font>2. Visualize Value Loss results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Loss\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"1dkrs5yx\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef79ff",
   "metadata": {},
   "source": [
    "<font>3. Visualize Value Accuracy Precision results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Accuracy Precision\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"296roujj\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225abdf8",
   "metadata": {},
   "source": [
    "<font>4. Visualize Training Loss results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"2k1f4hrp\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bca079",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Testing Section for TSU pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430af646",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### R5: Evaluate a trained model based on the TSU project.\n",
    "- Choose a dataset subfolder from the data folder to use for testing\n",
    "- Load a pretrained model \n",
    "- Run the testing sequence\n",
    "- Show the progress of testing in the notebook\n",
    "- View results to allow for an assessment of how well the model performed:\n",
    "    1. Average Precision per activity class \n",
    "    2. Mean Average Precision\n",
    "- Save the results to a results folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08621e64",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### US-16 & US-17\n",
    "\n",
    "<font color='green'>1. Load a list of pre-trained models for testing section.</font>\n",
    "<br>\n",
    "<font color='green'>2. Select a dataset subfolder from the data folder to use for testing.</font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a7a1a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Preparing UI to get user input\n",
    "container_1 = widgets.Box()\n",
    "container_2 = widgets.Box()\n",
    "# open trained_model directory and lsit trained models there\n",
    "# selectedModel.value will be the trained model name\n",
    "\n",
    "trained_model_dir = './trained_model/'\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = trained_model_dir\n",
    "selectedModel = FileChooser(starting_directory)\n",
    "selectedModel.title = '<b>Select a pretrained model to perform testing <b>'\n",
    "\n",
    "\n",
    "feature_data = './data/'\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = feature_data\n",
    "selectedFeatureData = FileChooser(starting_directory)\n",
    "selectedFeatureData.show_only_dirs = True\n",
    "selectedFeatureData.title = '<b>Select a dataset folder to perform testing <b>'\n",
    "\n",
    "\n",
    "\n",
    "control_1 = selectedModel\n",
    "control_2 = selectedFeatureData\n",
    "container_1.children = [control_1]\n",
    "container_2.children = [control_2]\n",
    "\n",
    "inputtabs = widgets.Tab()\n",
    "inputtabs.children = [container_1, container_2]\n",
    "inputtabs.set_title(0, \"Pre-trained Model\")\n",
    "inputtabs.set_title(1, \"Feature Data\")\n",
    "inputtabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681852c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Check your model testing configuration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad656335",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def inference(b):\n",
    "    print(\"Run next cell to start Testing.\")\n",
    "\n",
    "selectedFeatureDataPath = str(selectedFeatureData.selected_path)\n",
    "selectedFeature = selectedFeatureDataPath.split('/')\n",
    "selectedFeature = selectedFeature[len(selectedFeature)-1]\n",
    "selectedModelTest = selectedModel.selected.split('/')\n",
    "selectedModelTest = selectedModelTest[len(selectedModelTest)-1]\n",
    "\n",
    "print(\"------------------------------ SUMMARY OF TESTING RUN CONFIGURATION ------------------------------\")\n",
    "print(\"Running inference with...\")\n",
    "print(\"Model: \" + str(selectedModelTest))\n",
    "print(\"Feature Data: \" + selectedFeature)\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Confirm Config',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Confirm Configuration',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "button.on_click(inference)\n",
    "\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297eb25",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### US-18\n",
    "<font color='green'>Run testing sequences on data samples and record down the statistics</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbcd129",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Run test on model. (Only run on computer with GPU)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac6f55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!chmod 755 -R ./Toyota_Smarthome/pipline/run_PDAN.sh\n",
    "with open(\"./temp/stdout.txt\",\"wb\") as out, open(\"./temp/stderr.txt\",\"wb\") as err:\n",
    "    subprocess.Popen(\"./pipline/run_PDAN.sh %s %s\" % (str(selectedFeatureData.selected_path), str(selectedModel.selected)),stdout=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcf511",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Extract model evaluation results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39e112",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "mean_AP = 0.0\n",
    "per_class_AP = []\n",
    "extractClassAP = False\n",
    "\n",
    "# read output file \n",
    "with open (\"./temp/stdout.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        output.append(line)\n",
    "\n",
    "# Extract model evaluation results\n",
    "for line in output:\n",
    "    if \"rgb MAP\" in line:\n",
    "        mean_AP = float(line.replace(\"rgb MAP: \",\"\").replace(\")\\n\",\"\").replace(\"tensor(\", \"\"))\n",
    "    elif \"rgb per vid\" in line:\n",
    "        extractClassAP = True\n",
    "        classAP = line.replace(\"rgb per vid: \",\"\").replace(\"tensor([\",\"\").replace(\"\\n\",\"\").strip().split(\",\")\n",
    "        classAP = list(filter(None, classAP))\n",
    "        for ap in classAP:\n",
    "            per_class_AP.append(float(ap))\n",
    "    elif extractClassAP:\n",
    "        classAP = line.replace(\"\\n\",\"\").replace(\"]\",\"\").replace(\")\",\"\").strip().split(\",\")\n",
    "        classAP = list(filter(None, classAP))\n",
    "        for ap in classAP:\n",
    "            per_class_AP.append(float(ap))\n",
    "    \n",
    "extractClassAP = False\n",
    "\n",
    "# Ouput the results\n",
    "print(\"Model AP: \", mean_AP)\n",
    "print(\"Per class AP: \", per_class_AP)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0f120",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### US-20\n",
    "<font color='green'>Display the result of the completed test:</font>\n",
    "1. Average Precision per activity class\n",
    "2. Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18261fc2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Source: https://docs.wandb.ai/quickstart\n",
    "\n",
    "<font color='green'>Login to wandb for evalutaion result evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1045906",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwhitneytwh\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently logged in. Run next cell to see visualisation of Average Precision Per Activity Class.\n",
      "If the visualisation is not displaying, request account from Whitney.\n"
     ]
    }
   ],
   "source": [
    "if wandb.login():\n",
    "    print(\"Currently logged in. Run next cell to see visualisation of Average Precision Per Activity Class.\")\n",
    "    print(\"If the visualisation is not displaying, request account from Whitney.\")\n",
    "else:\n",
    "    print(\"Not logged in to any account.\")\n",
    "    print(\"If API key is required, request from Whitney.\")\n",
    "\n",
    "api = wandb.Api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567df05d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Visualize evalutated results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c1b41a",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/whitneytwh/precision-tables/runs/3pxhnhom?jupyter=true\" style=\"border:none;width:100%;height:720px;\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"3pxhnhom\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c62f4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### US-21\n",
    "<font color='green'>Save Average Precision results to a `./results` folder</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b77d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Create data folder if it does not exist \n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "#print(\"Today's date:\", today)\n",
    "\n",
    "result_directory = './results/'+str(selectedFeatureDataPath)+'_'+str(selectedModel.selected)+'_'+ str(today)\n",
    "\n",
    "if not os.path.exists(result_directory):\n",
    "    os.makedirs(result_directory)\n",
    "    \n",
    "print(\"Created Result folder:\" + str(result_directory))\n",
    "\n",
    "\n",
    "#resultFile = input(\"Name of File\")\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "resultFile = widgets.Text(\n",
    "    description='New Result File Name:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "resultFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2720a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "completeName = result_directory+'/'+resultFile.value+ \".txt\"         \n",
    "\n",
    "if not os.path.exists(completeName):\n",
    "    print(\"Created file:\" + completeName)\n",
    "    f=open(completeName, 'w')\n",
    "    f.write('average_precision = 0.3275')\n",
    "    f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e4945",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NVIDIA STEP model\n",
    "<font color='grey'>This section is for inference, training and testing using the STEP pipeline.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f12831",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inference Section for STEP pipeline\n",
    "<font color='green'>Select video input and model for inferencing. (Only P02T02C06 inferred, not enough GPU in colab to run any longer videos.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343e65f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set widget boxes for each variable we are loading, in this case, a pretrained model and a feature data.\n",
    "container_1 = widgets.Box()\n",
    "container_2 = widgets.Box()\n",
    "\n",
    "# open trained_model directory and lsit trained models there\n",
    "# selectedModel.value will be the trained model name\n",
    "trained_model_dir = './STEP-master/pretrained'\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = trained_model_dir\n",
    "selectedModel = FileChooser(starting_directory)\n",
    "selectedModel.title = '<b>Select a pretrained model to perform inference <b>'\n",
    "feature_data = './data/RGB_videos/'\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = feature_data\n",
    "selectedFeatureData = FileChooser(starting_directory)\n",
    "selectedFeatureData.title = '<b>Select a video folder to perform inferencing <b>'\n",
    "\n",
    "# Set variables to get selected iputs from user\n",
    "control_1 = selectedModel\n",
    "control_2 = selectedFeatureData\n",
    "container_1.children = [control_1]\n",
    "container_2.children = [control_2]\n",
    "\n",
    "# Load UI for user to select model and feature data\n",
    "inputtabs = widgets.Tab()\n",
    "inputtabs.children = [container_1, container_2]\n",
    "inputtabs.set_title(0, \"Pre-trained Model\")\n",
    "inputtabs.set_title(1, \"Feature Data\")\n",
    "inputtabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704e2b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>View inferencing configuration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d6e98",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make dir to store video frames if it doest not yet exist\n",
    "framePath = 'STEP-master/datasets/demo/frames/'\n",
    "videoNameArr = selectedFeatureData.value.split('/')\n",
    "videoName = videoNameArr[len(videoNameArr)-1].split('.')[0]\n",
    "storePath = framePath + videoName + \"/\"\n",
    "print(storePath)\n",
    "if not os.path.isdir(storePath):\n",
    "    os.mkdir(storePath)\n",
    "\n",
    "# Function to let user know what to do next after confirming configuration\n",
    "def inference(b):\n",
    "    print(\"Run next cell to start inferencing.\")\n",
    "\n",
    "# Set some variables to extract user selection to display user's current configuration\n",
    "selectedFeatureDataPath = str(selectedFeatureData.selected).split('/')\n",
    "selectedModelArr = selectedModel.selected.split('/')\n",
    "selectedModelName = selectedModelArr[len(selectedModelArr)-1]\n",
    "selectedFeature = selectedFeatureDataPath[len(selectedFeatureDataPath)-1]\n",
    "\n",
    "# Output user's configuration so that user has clearer idea of what they have selected\n",
    "print(\"------------------------------ SUMMARY OF INFERENCE RUN CONFIGURATION ------------------------------\")\n",
    "print(\"Running inference with...\")\n",
    "print(\"Model: \" + str(selectedModelName))\n",
    "print(\"Feature Data: \" + selectedFeature)\n",
    "\n",
    "# Button for user to confirm their confirmation\n",
    "button = widgets.Button(\n",
    "    description='Confirm Config',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Confirm Configuration',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Display button and allow user to confirm configuration\n",
    "button.on_click(inference)\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a18be7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Extract frames of video and store it in STEP-master for STEP pipeline input.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f96da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Setting 16 frames per second extraction\n",
    "fps = 16\n",
    "\n",
    "# Function to get the timedelta for naming of jpgs\n",
    "def format_timedelta(td):\n",
    "    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n",
    "    omitting microseconds and retaining milliseconds\"\"\"\n",
    "    result = str(td)\n",
    "    try:\n",
    "        result, ms = result.split(\".\")\n",
    "    except ValueError:\n",
    "        return result + \".00\".replace(\":\", \"-\")\n",
    "    ms = int(ms)\n",
    "    ms = round(ms / 1e4)\n",
    "    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n",
    "\n",
    "# Function to extract the frames from the video\n",
    "def extractFrames(video_file, storePath):\n",
    "    # load the video clip\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n",
    "    saving_frames_per_second = min(video_clip.fps, fps)\n",
    "    # if SAVING_FRAMES_PER_SECOND is set to 0, step is 1/fps, else 1/SAVING_FRAMES_PER_SECOND\n",
    "    step = 1 / video_clip.fps if saving_frames_per_second == 0 else 1 / saving_frames_per_second\n",
    "    # iterate over each possible frame\n",
    "    counter=0\n",
    "    for current_duration in np.arange(0, video_clip.duration, step):\n",
    "        # format the file name and save it\n",
    "        frame_duration_formatted = format_timedelta(timedelta(seconds=current_duration)).replace(\":\", \"-\")\n",
    "        #frame_filename = os.path.join(filename, f\"frame{frame_duration_formatted}.jpg\") /frame0000.jpg\n",
    "        frame_filename = os.path.join(storePath, f\"frame{str(counter).zfill(6)}.jpg\")\n",
    "        # save the frame with the current duration\n",
    "        counter += 1\n",
    "        video_clip.save_frame(frame_filename, current_duration)\n",
    "    print(\"Successfully extracted video frames. Please find frames in \", storePath)\n",
    "        \n",
    "extractFrames(selectedFeatureData.selected, storePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4df4cc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Run inference. (GPU only)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d5581",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# need specific cuda version\n",
    "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f503de",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#run this first before running demo\n",
    "!python STEP-master/setup.py build develop\n",
    "!python STEP-master/demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19069087",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Merge results to output video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5dd94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ground truth annotations\n",
    "#annotationsDir = 'Annotation/' + selectedFeature.split('.')[0][0:3] + '/' + selectedFeature.split('.')[0] + '.csv'\n",
    "'''\n",
    "# pipeline function to insert text into each prame\n",
    "def pipeline(frame):\n",
    "    try:\n",
    "        cv2.rectangle(frame, (50, 380), (250, 430), (0, 0, 0), -1)\n",
    "        cv2.rectangle(frame, (350, 380), (500, 430), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, str(\"ground truth\"), (50,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, str(next(captionsDF)[1].captions), (50,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, str(\"inferred results\"), (350,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1) \n",
    "        try:\n",
    "            cv2.putText(frame, str(next(inferredCaptions)[1].captions), (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        except:\n",
    "            cv2.putText(frame, str(inferredDF['captions'][len(inferredDF)-1]), (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    # additional frame manipulation\n",
    "    return frame\n",
    "\n",
    "#read ground truth data\n",
    "#df = pd.read_csv(annotationsDir)\n",
    "captionsByFrame = {'captions' : []}\n",
    "caption = \"\"\n",
    "totalEvents = len(df.index)\n",
    "oldStartFrame = 0\n",
    "\n",
    "#read inferred data\n",
    "inferredDF = pd.read_csv(inferredDir)\n",
    "\n",
    "#build new df\n",
    "for event in range(totalEvents):\n",
    "    #i need to minus old one\n",
    "    initialStartframe = int(df['start_frame'][event]) - oldStartFrame\n",
    "    for n in range(initialStartframe):\n",
    "        captionsByFrame['captions'].append(caption)\n",
    "    caption = str(df['event'][event])\n",
    "    oldStartFrame += initialStartframe\n",
    "    if (event == totalEvents-1):\n",
    "        initialStartframe = int(df['end_frame'][event]) - oldStartFrame\n",
    "        for n in range(initialStartframe):\n",
    "            captionsByFrame['captions'].append(caption)\n",
    "\n",
    "    \n",
    "captionsDF = pd.DataFrame(captionsByFrame).iterrows()\n",
    "inferredCaptions = inferredDF.iterrows()\n",
    "video = VideoFileClip(videoSelectedPath)\n",
    "out_video = video.fl_image(pipeline)\n",
    "\n",
    "# output path\n",
    "captionVideoPath = './video_withcaptions/' + selectedVideo.selected_filename\n",
    "out_video.write_videofile(captionVideoPath, audio=True)\n",
    "'''\n",
    "\n",
    "# Set path to read inferred results\n",
    "videoName = selectedFeature.split('.')[0]\n",
    "resultPath = 'STEP-master/datasets/demo/frames/results/'\n",
    "outputPath = resultPath + videoName + '/*.jpg'\n",
    "\n",
    "# Merge all ouputted images and output a video\n",
    "img_array = []\n",
    "for filename in glob.glob(outputPath):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width, height)\n",
    "    img_array.append(img)\n",
    "\n",
    "outPath = 'STEP-master/datasets/demo/results/'\n",
    "resultName = outPath + videoName + '.mp4'\n",
    "    \n",
    "# make output dir if it does not exist\n",
    "if not os.path.isdir(outPath):\n",
    "    os.mkdir(outPath)\n",
    "    \n",
    "\n",
    "image_files = [os.path.join(resultPath+videoName, img)\n",
    "              for img in os.listdir(resultPath+videoName)\n",
    "              if img.endswith(\".jpg\")]\n",
    "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=16)\n",
    "clip.write_videofile('STEP-master/datasets/demo/results/test.mp4')\n",
    "\n",
    "print(\"Successfully merged video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda8a56",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Play the video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ead07f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#resultName = './' + resultName\n",
    "print(resultName)\n",
    "Video('STEP-master/datasets/demo/results/test.mp4', embed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767651d5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training Section for STEP pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb211e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'>Set up and infer using STEP pipeline. (Need GPU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63bf6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color='green'> (Need GPU)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c78817",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert csv to pkl for training and testing\n",
    "!python STEP-master/scripts/generate_label.py \"STEP-master/datasets/generate_label/ava_train_v2.2.csv\"\n",
    "!python STEP-master/scripts/generate_label.py \"STEP-master/datasets/generate_label/ava_val_v2.2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ebf83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate frames for testing\n",
    "!python STEP-master/scripts/extract_clips.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ea64f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
