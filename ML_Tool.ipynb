{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f22876b2",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# ML TOOL \n",
    "\n",
    "<span> A jupyter notebook tool to aid the testing of machine learning (ML) models in the domain of activity detection.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d52b7",
   "metadata": {},
   "source": [
    "Source: https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\n",
    "<div></div>\n",
    "Source: https://jupyter-contrib-nbextensions.readthedocs.io/en/latest/install.html\n",
    "<div></div>\n",
    "<font color='green'>Run the cell below to enable minimization of code blocks for better looking UI.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4163fb",
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "%%HTML \n",
    "<script>\n",
    "\n",
    "    function luc21893_refresh_cell(cell) {\n",
    "        if( cell.luc21893 ) return;\n",
    "        cell.luc21893 = true;\n",
    "        console.debug('New code cell found...' );\n",
    "        \n",
    "        var div = document.createElement('DIV');            \n",
    "        cell.parentNode.insertBefore( div, cell.nextSibling );\n",
    "        div.style.textAlign = 'right';\n",
    "        var a = document.createElement('A');\n",
    "        div.appendChild(a);\n",
    "        a.href='#'\n",
    "        a.luc21893 = cell;\n",
    "        a.setAttribute( 'onclick', \"luc21893_toggle(this); return false;\" );\n",
    "\n",
    "        cell.style.visibility='hidden';\n",
    "        cell.style.position='absolute';\n",
    "        a.innerHTML = '[show code]';        \n",
    "                \n",
    "    }\n",
    "    function luc21893_refresh() {                \n",
    "        if( document.querySelector('.code_cell .input') == null ) {            \n",
    "            // it apeears that I am in a exported html\n",
    "            // hide this code\n",
    "            var codeCells = document.querySelectorAll('.jp-InputArea')\n",
    "            codeCells[0].style.visibility = 'hidden';\n",
    "            codeCells[0].style.position = 'absolute';                        \n",
    "            for( var i = 1; i < codeCells.length; i++ ) {\n",
    "                luc21893_refresh_cell(codeCells[i].parentNode)\n",
    "            }\n",
    "            window.onload = luc21893_refresh;\n",
    "        }                 \n",
    "        else {\n",
    "            // it apperas that I am in a jupyter editor\n",
    "            var codeCells = document.querySelectorAll('.code_cell .input')\n",
    "            for( var i = 0; i < codeCells.length; i++ ) {\n",
    "                luc21893_refresh_cell(codeCells[i])\n",
    "            }            \n",
    "            window.setTimeout( luc21893_refresh, 1000 )\n",
    "        }        \n",
    "    }\n",
    "    \n",
    "    function luc21893_toggle(a) {\n",
    "        if( a.luc21893.style.visibility=='hidden' ) {\n",
    "            a.luc21893.style.visibility='visible';        \n",
    "            a.luc21893.style.position='';\n",
    "            a.innerHTML = '[hide code]';\n",
    "        }\n",
    "        else {\n",
    "            a.luc21893.style.visibility='hidden';        \n",
    "            a.luc21893.style.position='absolute';\n",
    "            a.innerHTML = '[show code]';\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    luc21893_refresh()\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea6a9ea",
   "metadata": {},
   "source": [
    "## Dependencies installation\n",
    "\n",
    "<font color='green'>Run this cell to install all required libraries.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59cb734-9a3e-4927-951b-34f48710f156",
   "metadata": {
    "hide_input": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/ehl/torch_stable.html\n",
    "%pip install timm==0.4.12\n",
    "%pip install scikit-learn\n",
    "%pip install numpy\n",
    "%pip install ipywidgets==7.4.2\n",
    "%pip install tqdm\n",
    "%pip install pandas\n",
    "%pip install moviepy\n",
    "%pip install opencv-python\n",
    "%pip install jupyter_contrib_nbextensions\n",
    "%pip install opencv-python\n",
    "'''\n",
    "if you are having issues installing moviepy, do \n",
    "%pip install --upgrade pip\n",
    "then \n",
    "%pip install moviepy\n",
    "'''\n",
    "\n",
    "# Installation of required Libraries \n",
    "!pip install ipywidgets\n",
    "!pip install jupyter-js-widgets-nbextension\n",
    "!jupyter nbextension enable --py --sys-prefix widgetsnbextension\n",
    "!pip install ipyfilechooser\n",
    "!pip install wandb\n",
    "!pip install wandb -qU\n",
    "!jupyter contrib nbextension install --user\n",
    "! pip install omegaconf==2.0.6\n",
    "\n",
    "print(\"All dependency installed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fadfc0-0020-455b-b1e6-910a8b72209c",
   "metadata": {},
   "source": [
    "## Import all Dependencies and Setup\n",
    "<font color='green'>Run this cell to import all required libraries.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7d1a5b-c24b-4b2a-9081-4f0a1c504a8f",
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import wandb\n",
    "import itertools  \n",
    "import subprocess\n",
    "import json\n",
    "from os import walk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from subprocess import Popen\n",
    "from datetime import timedelta\n",
    "from ipyfilechooser import FileChooser\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import Video\n",
    "\n",
    "print(\"Running on touch version \" + torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094899c0",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e892b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### R1: An interactive python Jupyter notebook in a github repository\n",
    "- Ensure jupyter notebook has python3.x \n",
    "- Locally interact with the notebook on a local linux, mac or windows environment that has python 3.x, Jupyter and other necessary dependencies installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a1b8c",
   "metadata": {},
   "source": [
    "#### US-01\n",
    "<font color='green'>Run this cell to ensure notebook is running on python 3.8.x.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d9fb6-dec4-4ed3-8fa2-2c2cfbd5ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(\"Running on Python Version \" + python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b247b960",
   "metadata": {},
   "source": [
    "### R2.: Data Exploration Section\n",
    "- A data folder in the repo with subfolders that store input video files and other related input files for different types of datasets\n",
    "- Choose a video file from the data folder through an appropriate UI component (e.g., dropdown menu) in a notebook code cell\n",
    "- See video playback of the chosen video file in an output cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a418e7",
   "metadata": {},
   "source": [
    "#### US-03\n",
    "<font color='green'>Run this cell to create to ensure `./data` folder is created.</font>\n",
    "<br>\n",
    "<font color='green'>Sub-folder `./data/RGB_videos` folder is created to store **Toyota Smarthome dataset**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850b1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data folder if it does not exist \n",
    "data_directory = Path('./data')\n",
    "if not os.path.exists(data_directory):\n",
    "    os.makedirs(data_directory)\n",
    "    \n",
    "# Create RGB video subfoler \n",
    "directory = data_directory/\"RGB_videos\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(\"Created \" + str(directory))\n",
    "else:\n",
    "    print(\"Sub-folder \" +str(directory) + \" exist\")\n",
    "    \n",
    "# Check if videos exist in subfolder \n",
    "if os.path.isdir(directory) and os.path.exists(directory):\n",
    "    if len(os.listdir(directory)) == 0:\n",
    "        print(\" \")\n",
    "        print(str(directory) + \" is empty\")\n",
    "        print(\"Please upload video data to this file path: \"+ str(directory))\n",
    "    else:    \n",
    "        print(\" \")\n",
    "        print(str(directory) + \" is not empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256964f",
   "metadata": {},
   "source": [
    "#### US-04 & US-05\n",
    "<font color='green'>Select Video to playback.</font>\n",
    "<div></div>\n",
    "<font color='green'>Run this cell to select video input for data exploration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a folder from the datasets folder created\n",
    "starting_directory = os.listdir(data_directory)\n",
    "folderSelect = widgets.Dropdown(\n",
    "    options=starting_directory,\n",
    "    value= starting_directory[0],\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "folderSelect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display only MP4 files\n",
    "videos = []\n",
    "for file in glob.glob(\"./data/\"+ folderSelect.value +\"/*.mp4\"):\n",
    "    filename = os.path.basename(file)\n",
    "    videos.append(filename)\n",
    "    \n",
    "    \n",
    "# Select a video file\n",
    "fileSelect =widgets.Select(\n",
    "    options=videos,\n",
    "    # rows=10,\n",
    "    description=folderSelect.value,\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "fileSelect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f820ac",
   "metadata": {},
   "source": [
    "<font color='green'>Run this cell to play selected video.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3605cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = Path(\"./data/\"+ folderSelect.value +'/'+fileSelect.value )\n",
    "print(\"Displaying video:\",videoPath)\n",
    "# Play the video from selected local path using video()\n",
    "Video(videoPath, embed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce27c1",
   "metadata": {},
   "source": [
    "# TSU Pipeline\n",
    "\n",
    "<font color='grey'>This section is for inference, training and testing using the TSU pipeline.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517176f9",
   "metadata": {},
   "source": [
    "## Feature Extraction \n",
    "\n",
    "<font color='grey'>Extract video features from raw videos using multiple GPUs using v-iashin repository. We will be using RAFT flow frames as well as I3D features.</font>\n",
    "\n",
    "Source: https://github.com/v-iashin/video_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9869dd",
   "metadata": {},
   "source": [
    "### R10: Prepare for Extraction\n",
    "- Clone the v-iashin repository\n",
    "- Create required folders to store the RGB, RGB+FLOW and FLOW extracted files\n",
    "- Check cuda device available and extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------------------- SUMMARY OF FOLDER CREATION -------------------------------------------\")\n",
    "# Create data folder if it does not exist \n",
    "RGB_directory = Path('./data/RGB_i3d_16frames_extracted/')\n",
    "if not os.path.exists(RGB_directory):\n",
    "    os.makedirs(RGB_directory)\n",
    "    print(\"Created \" + str(RGB_directory))\n",
    "else:\n",
    "    print(str(RGB_directory) + \" exist\") \n",
    "    \n",
    "# Create data folder if it does not exist \n",
    "FLOW_directory = Path('./data/FLOW_i3d_16frames_extracted/')\n",
    "if not os.path.exists(FLOW_directory):\n",
    "    os.makedirs(FLOW_directory)\n",
    "    print(\"Created \" + str(FLOW_directory))\n",
    "else:\n",
    "    print(str(FLOW_directory) + \" exist\") \n",
    "    \n",
    "# Create data folder if it does not exist \n",
    "FLOWRGB_directory = Path('./data/FLOWnRGB_i3d_16frames_extracted/')\n",
    "if not os.path.exists(FLOWRGB_directory):\n",
    "    os.makedirs(FLOWRGB_directory)\n",
    "    print(\"Created \" + str(FLOWRGB_directory) +\"\\n\\n\")\n",
    "else:\n",
    "    print(str(FLOWRGB_directory) + \" exist\\n\\n\") \n",
    "    \n",
    "repo = Path('./video_features/')\n",
    "# Clone the v-iashin repo\n",
    "if not os.path.exists(repo):\n",
    "    ! git clone https://github.com/v-iashin/video_features.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c24b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./video_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17fe37",
   "metadata": {},
   "source": [
    "#### US-28\n",
    "<font>**Extract** features **using I3D feature** folder to RGB, FLOW and RGB+FLOW **NPY files** (**GPU only**).  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3910dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont run this cell if u no CUDA!!!\n",
    "from models.i3d.extract_i3d import ExtractI3D\n",
    "from utils.utils import build_cfg_path\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device is \" + device)\n",
    "    print(\"CUDA device name:\")\n",
    "    torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2929844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = '../data/RGB_videos/'\n",
    "\n",
    "onlyfiles = [mypath+f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(\"Raw video file that are going to be extracted:\\n\" + str(onlyfiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f62a66f",
   "metadata": {},
   "source": [
    "##### RGB (GPU needed)\n",
    "<font >**Extract** files are stored in `./data/RGB_i3d_16frames_extracted/` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Select the feature type\n",
    "    feature_type = 'i3d'\n",
    "\n",
    "    # Load and patch the config\n",
    "    args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "    args.video_paths = onlyfiles\n",
    "    # args.show_pred = True\n",
    "    # args.stack_size = 24\n",
    "    # args.step_size = 24\n",
    "    args.extraction_fps = 16\n",
    "    args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
    "    args.streams = 'rgb'\n",
    "    #args.on_extraction = 'save_numpy'\n",
    "    #args.output_path = './sample/Output'\n",
    "\n",
    "\n",
    "    # Load the model\n",
    "    extractor = ExtractI3D(args)\n",
    "\n",
    "    # Extract features\n",
    "    for video_path in args.video_paths:\n",
    "        print(f'Extracting for {video_path}')\n",
    "        base=os.path.basename(video_path)\n",
    "        fileName = os.path.splitext(base)[0]\n",
    "\n",
    "        feature_dict = extractor.extract(video_path)\n",
    "        #[(print(k), print(v.shape), print(v)) for k, v in feature_dict.items()]\n",
    "        extracted_data = list(feature_dict.values())[0]\n",
    "        #print(extracted_data)\n",
    "        save(\"../data/RGB_i3d_16frames_extracted/\"+fileName+\".npy\",extracted_data)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a32f9",
   "metadata": {},
   "source": [
    "##### FLOW (GPU needed)\n",
    "<font>**Extract** files are stored in `./data/FLOW_i3d_16frames_extracted/` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ae89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import os\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Select the feature type\n",
    "    feature_type = 'i3d'\n",
    "\n",
    "    # Load and patch the config\n",
    "    args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "    args.video_paths = onlyfiles\n",
    "    # args.show_pred = True\n",
    "    # args.stack_size = 24\n",
    "    # args.step_size = 24\n",
    "    args.extraction_fps = 16\n",
    "    args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
    "    args.streams = 'flow'\n",
    "    #args.on_extraction = 'save_numpy'\n",
    "    #args.output_path = './sample/Output'\n",
    "\n",
    "\n",
    "    # Load the model\n",
    "    extractor = ExtractI3D(args)\n",
    "\n",
    "    # Extract features\n",
    "    for video_path in args.video_paths:\n",
    "        print(f'Extracting for {video_path}')\n",
    "        base=os.path.basename(video_path)\n",
    "        fileName = os.path.splitext(base)[0]\n",
    "\n",
    "        feature_dict = extractor.extract(video_path)\n",
    "        #[(print(k), print(v.shape), print(v)) for k, v in feature_dict.items()]\n",
    "        extracted_data = list(feature_dict.values())[0]\n",
    "        #print(extracted_data)\n",
    "        save(\"../data/FLOW_i3d_16frames_extracted/\"+fileName+\".npy\",extracted_data)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8131f12",
   "metadata": {},
   "source": [
    "##### FLOW + RGB (GPU needed)\n",
    "<font>**Extract** files are stored in `./data/FLOWnRGB_i3d_16frames_extracted/` </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb1bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import save\n",
    "\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Select the feature type\n",
    "    feature_type = 'i3d'\n",
    "\n",
    "    # Load and patch the config\n",
    "    args = OmegaConf.load(build_cfg_path(feature_type))\n",
    "    args.video_paths = onlyfiles\n",
    "    # args.show_pred = True\n",
    "    # args.stack_size = 24\n",
    "    # args.step_size = 24\n",
    "    args.extraction_fps = 16\n",
    "    args.flow_type = 'raft' # 'pwc' is not supported on Google Colab (cupy version mismatch)\n",
    "    #args.streams = 'rgb'\n",
    "    #args.on_extraction = 'save_numpy'\n",
    "    #args.output_path = './sample/Output'\n",
    "\n",
    "\n",
    "    # Load the model\n",
    "    extractor = ExtractI3D(args)\n",
    "\n",
    "\n",
    "    # Extract features\n",
    "    for video_path in args.video_paths:\n",
    "        print(f'Extracting for {video_path}')\n",
    "        base=os.path.basename(video_path)\n",
    "        fileName = os.path.splitext(base)[0]\n",
    "\n",
    "        feature_dict = extractor.extract(video_path)\n",
    "        #[(print(k), print(v.shape), print(v)) for k, v in feature_dict.items()]\n",
    "        extracted_rgb = list(feature_dict.values())[0]\n",
    "        extracted_flow = list(feature_dict.values())[1]\n",
    "        comb = np.column_stack((extracted_rgb, extracted_flow))\n",
    "        np.save(\"../data/FLOWnRGB_i3d_16frames_extracted/\"+fileName+\".npy\",comb)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc59d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Return to main folder...\")\n",
    "#if \n",
    "#%cd ./T01-nvidia-jupyternotebookenv\n",
    "\n",
    "# get current directory\n",
    "path = os.getcwd()\n",
    "base=os.path.basename(path)\n",
    "fileName = os.path.splitext(base)[0]\n",
    "#print(fileName)\n",
    "\n",
    "if fileName == \"video_features\":\n",
    "    %cd ..\n",
    "    \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b3798",
   "metadata": {},
   "source": [
    "## Inference Section for TSU pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c27062",
   "metadata": {},
   "source": [
    "### R3: Perform inference using a pretrained model based on the TSU project\n",
    "- Ensure TSU Pipeline is able to run with multiple numpy files in the same dataset directory \n",
    "- Load a pretrained model using an appropriate UI component\n",
    "- Choose an input video, using an appropriate UI component, from the TSU project\n",
    "- See inference results in the form of an output video with captions that indicate the current detected activity in each video frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff2454f",
   "metadata": {},
   "source": [
    "#### US-06 \n",
    "<font>**Load** a **pretrained model** and extracted **feature data** folder to perform inference. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0994a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set widget boxes for each variable we are loading, in this case, a pretrained model and a feature data.\n",
    "container_1 = widgets.Box()\n",
    "container_2 = widgets.Box()\n",
    "\n",
    "# open trained_model directory and lsit trained models there\n",
    "# selectedModel.value will be the trained model name\n",
    "trained_model_dir = './trained_model/'\n",
    "models =  os.listdir(trained_model_dir)\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "selectedModel =widgets.Select(\n",
    "    options=models,\n",
    "    # rows=10,\n",
    "    description=\"Models:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = os.listdir(data_directory)\n",
    "selectedFeatureData = widgets.Dropdown(\n",
    "    options=starting_directory,\n",
    "    value= starting_directory[0],\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Set variables to get selected iputs from user\n",
    "control_1 = selectedModel\n",
    "control_2 = selectedFeatureData\n",
    "container_1.children = [control_1]\n",
    "container_2.children = [control_2]\n",
    "\n",
    "# Load UI for user to select model and feature data\n",
    "inputtabs = widgets.Tab()\n",
    "inputtabs.children = [container_1, container_2]\n",
    "inputtabs.set_title(0, \"Pre-trained Model\")\n",
    "inputtabs.set_title(1, \"Feature Data\")\n",
    "inputtabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset = './data/' + selectedFeatureData.value\n",
    "chosen_model = trained_model_dir+selectedModel.value\n",
    "\n",
    "# Function to let user know what to do next after confirming configuration\n",
    "def inference(b):\n",
    "    print(\"Run next cell to start inferencing.\")\n",
    "\n",
    "\n",
    "# Output user's configuration so that user has clearer idea of what they have selected\n",
    "print(\"------------------------------ SUMMARY OF INFERENCE RUN CONFIGURATION ------------------------------\")\n",
    "print(\"Running inference with...\")\n",
    "print(\"Model: \" + selectedModel.value)\n",
    "print(\"Feature Data: \" + selectedFeatureData.value)\n",
    "\n",
    "# Button for user to confirm their confirmation\n",
    "button = widgets.Button(\n",
    "    description='Confirm Config',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Confirm Configuration',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Display button and allow user to confirm configuration\n",
    "button.on_click(inference)\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438f6ae",
   "metadata": {},
   "source": [
    "#### US-36\n",
    "<font>The list of **numpy files** that are in ``./data/____``, match them to existing entries in the JSON </font>\n",
    "- If not present in the directory, eliminate the entry from the JSON file. \n",
    "- If multiple numpy files starting with the same name, a duplicate the same original entry in the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkKey(dic, key):\n",
    "    if key in dic.keys():\n",
    "        return True \n",
    "    else:\n",
    "        #print(key +\": Not present\")\n",
    "        return False\n",
    "        \n",
    "        \n",
    "def checkJSONFile(jsonFile, onlyfiles):\n",
    "    # Opening JSON file\n",
    "    readFile = open(jsonFile)\n",
    "    # returns JSON object as \n",
    "    # a dictionary\n",
    "    jsonData = json.load(readFile)\n",
    "    fileNameOnly = []\n",
    "\n",
    "    # If multiple numpy files starting with the same name, a duplicate the same original entry in the JSON\n",
    "    for i in onlyfiles:\n",
    "        key = i.split('.')\n",
    "        fileNameOnly.append(key[0])\n",
    "        if checkKey(jsonData, key[0]):\n",
    "            pass \n",
    "        else:\n",
    "            new_key = key[0]\n",
    "            old_key = key[0].split('_')\n",
    "            print(old_key[0])\n",
    "            if(checkKey(jsonData, old_key[0])):\n",
    "                newData = {new_key: jsonData[old_key[0]]}\n",
    "                # appending the data\n",
    "                jsonData.update(newData)\n",
    "                print(\"Added: \" + str(newData) + \" to \" + jsonFile)\n",
    "\n",
    "    # If not present in the directory, eliminate the entry from the JSON file.\n",
    "    deleteList = []\n",
    "    for i in jsonData.keys():\n",
    "        if i in fileNameOnly:\n",
    "            pass\n",
    "        else:\n",
    "            deleteList.append(i)\n",
    "\n",
    "    for i in deleteList:\n",
    "        jsonData.pop(i)\n",
    "        print(\"Deleted: \" + i + \" from \" + jsonFile)\n",
    "    \n",
    "    \n",
    "    # 3. Write json file\n",
    "    with open(jsonFile, \"w\") as file:\n",
    "        json.dump(jsonData, file) \n",
    "        print(\"Re-written: \" + jsonFile)\n",
    "\n",
    "\n",
    "# read all files in dataset folder (selected by user)\n",
    "datasetPath = Path(chosen_dataset)\n",
    "onlyfiles = []\n",
    "for (dirpath, dirnames, filenames) in walk(datasetPath):\n",
    "    onlyfiles.extend(filenames)\n",
    "    break\n",
    "    \n",
    "    \n",
    "    \n",
    "jsonFile1 = './Toyota_Smarthome/pipline/data/smarthome_CS_51.json'    \n",
    "jsonFile2 = './Toyota_Smarthome/pipline/data/smarthome_CV_51.json'    \n",
    "\n",
    "checkJSONFile(jsonFile1, onlyfiles)\n",
    "checkJSONFile(jsonFile2, onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 755 -R ./Toyota_Smarthome/pipline/run_PDAN.sh\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Run using Popen and output to appropriate files for diaply later\n",
    "    with open(\"./temp/stdout.txt\",\"wb\") as out, open(\"./temp/stderr.txt\",\"wb\") as err:\n",
    "        subprocess.Popen(\"./Toyota_Smarthome/pipline/run_PDAN.sh\", str(chosen_dataset), str(chosen_model),stdout=out)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340cd9f",
   "metadata": {},
   "source": [
    "<font>Extract key values from inference result for each video, map them to the appropriate actions and output them to a csv for captions injection.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb57a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output array to read output from popen function\n",
    "output = []\n",
    "\n",
    "# set path variables to open and read line from files\n",
    "resultOutputPath = \"./Annotation/results/\"\n",
    "csvOutputName = resultOutputPath\n",
    "\n",
    "# Read the the key to action mapping file\n",
    "actionMapDataFrame = pd.read_csv(\"./Annotation/labels.csv\")\n",
    "print(\"Reading labels from ./Annotation/labels.csv\")\n",
    "\n",
    "# Creation of dataframe for inferred captions\n",
    "captionsByFrame = {'captions' : []}\n",
    "#print(actionMapDataFrame[\"Event\"][0]) get 0 key action\n",
    "# open inference results, input to array\n",
    "print(\"Reading inference reults from ./temp/stdout.txt\")\n",
    "\n",
    "# open file and read line each to build output array data structure\n",
    "with open (\"./temp/stdout.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        output.append(line)\n",
    "   \n",
    "# map each action element to its label, multiply by 16frames and output into csv\n",
    "for line in output:\n",
    "    if \"video\" in line:\n",
    "        csvOutputName = resultOutputPath\n",
    "        csvOutputName += line.split(\":\")[1].replace('\\n',\"\").replace(\" \",\"\") + \".csv\"\n",
    "        # reset captions for new video\n",
    "        captionsByFrame = {'captions' : []}\n",
    "        \n",
    "    else:\n",
    "        #actions:  [ 0  0  0 ... 26 26 26] end\n",
    "        if \"actions\" in line and \"end\" in line:\n",
    "            actions = line.replace(\"actions: \",\"\").replace(\"end\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"\\n\",\"\").replace(\"...\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            \n",
    "            for action in actions:\n",
    "                actionKey = int(action)\n",
    "                for frame in range(16):\n",
    "                     captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "\n",
    "            captionsDF = pd.DataFrame(captionsByFrame)\n",
    "            captionsDF.to_csv(csvOutputName) \n",
    "\n",
    "        elif \"actions\" in line:\n",
    "            #actions:  [ 1  0  0  0  0  0  0  0  1  1  1  1  1  1  1 16  1  1 22 22 21 22 22 22\n",
    "            actions = line.replace(\"actions: \",\"\").replace(\"[\",\"\").replace(\"\\n\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            for action in actions:\n",
    "                actionKey = int(action)\n",
    "                for frame in range(16):\n",
    "                     captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "                        \n",
    "        elif \"evaluation\" not in line and \"rgb\" not in line and \"end\" not in line:\n",
    "            if line == \"1\\n\" or line == \"2\\n\":\n",
    "                continue\n",
    "            actions = line.replace(\"\\n\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            for action in actions:\n",
    "                try:\n",
    "                    actionKey = int(action)\n",
    "\n",
    "                    for frame in range(16):\n",
    "                         captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "                except:\n",
    "                    pass\n",
    "        elif \"end\" in line:\n",
    "            actions = line.replace(\"]\",\"\").replace(\"end\\n\",\"\").split(\" \")\n",
    "            actions = list(filter(None, actions))\n",
    "            for action in actions:\n",
    "\n",
    "                actionKey = int(action)\n",
    "\n",
    "                for frame in range(16):\n",
    "                    captionsByFrame[\"captions\"].append(actionMapDataFrame[\"Event\"][actionKey])\n",
    "\n",
    "            captionsDF = pd.DataFrame(captionsByFrame)\n",
    "            captionsDF.to_csv(csvOutputName)   \n",
    "            \n",
    "print(\" Inferred captions successfully mapped to actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f761b35",
   "metadata": {},
   "source": [
    "#### US-07\n",
    "Create annotations file if it does not exist. </font>\n",
    "<br><br>\n",
    "<font>Select an input video from the TSU project to output video with captions based on annotations. (use P02T16C06) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for reading and user selection of captioned videos\n",
    "videoDir = 'data/RGB_videos/'\n",
    "inferredList = 'Annotation/results/'\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = videoDir\n",
    "selectedVideo = FileChooser(starting_directory)\n",
    "\n",
    "# Display only MP4 files\n",
    "videos = []\n",
    "for file in glob.glob(\"./data/RGB_videos/*.mp4\"):\n",
    "    filename = os.path.basename(file)\n",
    "    videos.append(filename)\n",
    "    \n",
    "    \n",
    "    \n",
    "selectedVideo =widgets.Select(\n",
    "    options=videos,\n",
    "    # rows=10,\n",
    "    description=\"RGB Videos:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "selectedVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user selcted video and store in a variable\n",
    "videoSelectedPath = \"./data/RGB_videos/\"+selectedVideo.value \n",
    "annotationsDir = 'Annotation/' + selectedVideo.value[0:3] + '/' + selectedVideo.value.split('.')[0] + '.csv'\n",
    "inferredDir = inferredList + selectedVideo.value.split('.')[0] + '.csv'\n",
    "\n",
    "# print user selection and configuration\n",
    "print(\"--------------------------------- SUMMARY OF SELECTED INFERNCE ---------------------------------\")\n",
    "#print(\"Ground Truth: \" + annotationsDir)\n",
    "#print(\"Inferred: \" + inferredDir)\n",
    "#print(\"\")\n",
    "print(f\"You have selected to view inference results for video '{selectedVideo.value}'. You will see a video comparing ground truth captions and inferred captions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eff59a",
   "metadata": {},
   "source": [
    "<font>Inject captions into each frame of the video and output captioned video for display.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934d38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline function to insert text into each prame\n",
    "def pipeline(frame):\n",
    "    gtCaptions = \"\"\n",
    "    ifCaptions = \"\"\n",
    "    try:\n",
    "        gtCaptions = str(next(captionsDF)[1].captions)\n",
    "        try:\n",
    "            ifCaptions = str(next(inferredCaptions)[1].captions)\n",
    "        except:\n",
    "            ifCaptions = str(inferredDF['captions'][len(inferredDF)-1])\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    gtcolor = (0, 255, 0)\n",
    "    ifcolor = (255, 0, 0)\n",
    "    if (gtCaptions == ifCaptions):\n",
    "        ifcolor = (0, 255, 0)\n",
    "        \n",
    "    try:\n",
    "        cv2.rectangle(frame, (50, 380), (250, 430), (0, 0, 0), -1)\n",
    "        cv2.rectangle(frame, (350, 380), (550, 430), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, str(\"GROUND TRUTH\"), (50,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.putText(frame, gtCaptions, (50,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, gtcolor, 1)\n",
    "        cv2.putText(frame, str(\"INFERRED RESULTS\"), (350,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1) \n",
    "        cv2.putText(frame, ifCaptions, (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, ifcolor, 1)\n",
    "        \n",
    "    except StopIteration:\n",
    "        pass\n",
    "    # additional frame manipulation\n",
    "    return frame\n",
    "\n",
    "#read ground truth data\n",
    "df = pd.read_csv(annotationsDir)\n",
    "captionsByFrame = {'captions' : []}\n",
    "caption = \"\"\n",
    "totalEvents = len(df.index)\n",
    "oldStartFrame = 0\n",
    "\n",
    "#read inferred data\n",
    "inferredDF = pd.read_csv(inferredDir)\n",
    "\n",
    "#build new df\n",
    "for event in range(totalEvents):\n",
    "    #i need to minus old one\n",
    "    initialStartframe = int(df['start_frame'][event]) - oldStartFrame\n",
    "    for n in range(initialStartframe):\n",
    "        captionsByFrame['captions'].append(caption)\n",
    "    caption = str(df['event'][event])\n",
    "    oldStartFrame += initialStartframe\n",
    "    if (event == totalEvents-1):\n",
    "        initialStartframe = int(df['end_frame'][event]) - oldStartFrame\n",
    "        for n in range(initialStartframe):\n",
    "            captionsByFrame['captions'].append(caption)\n",
    "\n",
    "    \n",
    "captionsDF = pd.DataFrame(captionsByFrame).iterrows()\n",
    "inferredCaptions = inferredDF.iterrows()\n",
    "video = VideoFileClip(videoSelectedPath)\n",
    "out_video = video.fl_image(pipeline)\n",
    "\n",
    "# output path\n",
    "captionVideoPath = './video_withcaptions/' + selectedVideo.value\n",
    "out_video.write_videofile(captionVideoPath, audio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3196cb8c",
   "metadata": {},
   "source": [
    "#### US-08\n",
    "<font>Play output video with captions.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(captionVideoPath, embed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9465c",
   "metadata": {},
   "source": [
    "## Training Section for TSU pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86762d",
   "metadata": {},
   "source": [
    "### R4: Train a HOI ML model based on the TSU project.\n",
    "- Choose a dataset subfolder to use for the training\n",
    "- Initialize a model (to be trained) with a network architecture configured in a separate .py file\n",
    "- Specify a name for this new model\n",
    "- Set the batch_size and epochs\n",
    "- Run the training sequence\n",
    "- Show progress of the training in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104394a",
   "metadata": {},
   "source": [
    "#### US-09, US-11 & US-12\n",
    "\n",
    "<font >1. Select a dataset subfolder from the data folder to use for training.</font>\n",
    "<br>\n",
    "<font >2. Specify a name for the new model.</font>\n",
    "<br>\n",
    "<font >3. Set the batch_size and epochs.</font>\n",
    "<br>\n",
    "<font >4. Check training configuration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model path if dont exist\n",
    "modelPath = Path('./trained_model')\n",
    "load_model = \"False\"\n",
    "# Created output model dir if it does not exist\n",
    "if not os.path.exists(modelPath):\n",
    "    os.makedirs(modelPath)\n",
    "    #print(\"Created \" + str(modelPath))\n",
    "else:\n",
    "    #print(\"Folder \" + str(modelPath) + \" exist\")\n",
    "    pass\n",
    "    \n",
    "\n",
    "# Create UI for user selection for training the model\n",
    "container_1 = widgets.VBox()\n",
    "container_2 = widgets.Box()\n",
    "container_3 = widgets.Box()\n",
    "\n",
    "out = widgets.Output()\n",
    "config_checks_output = widgets.Output()\n",
    "\n",
    "with out:\n",
    "    print(\"Current Selected Feature Data Path:\" + str(selectedFeatureData.value) + \"\\nSelect a new folder if you wish to change it\" )\n",
    "\n",
    "\n",
    "  \n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = os.listdir(data_directory)\n",
    "dataset = widgets.Dropdown(\n",
    "    options=starting_directory,\n",
    "    value= starting_directory[0],\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "modelPath = \"./trained_model/\"\n",
    "newModel = widgets.Text(\n",
    "    description='New Model Name:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "epoch = widgets.Text(\n",
    "    description='Epoch Value:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "batch_size = widgets.Text(\n",
    "    description='Batch Size:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "button_submit = widgets.Button(\n",
    "    description='Submit Configs',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Click to Submit Training Configurations'\n",
    ")\n",
    "\n",
    "control_1 = dataset\n",
    "control_5 = out\n",
    "control_2 = newModel\n",
    "control_3 = epoch\n",
    "control_4 = batch_size\n",
    "container_1.children = [control_5, control_1]\n",
    "container_2.children = [control_2]\n",
    "container_3.children = [control_3, control_4]\n",
    "\n",
    "accordions = widgets.Accordion()\n",
    "accordions.children = [container_1, container_2, container_3]\n",
    "accordions.set_title(0, \"Dataset\")\n",
    "accordions.set_title(1, \"Output Model\")\n",
    "accordions.set_title(2, \"Batch_size & Epochs\")\n",
    "\n",
    "def preview_training_config(newModel, epoch, batch_size):\n",
    "    print(\"\")\n",
    "    print(\"--------------------------------- SUMMARY OF TRAINING PARAMETERS ---------------------------------\")\n",
    "    print(\"Pipeline: TSU pipeline\")\n",
    "    if dataset.value is not None:\n",
    "        print(\"Feature Data: \" + dataset.value)\n",
    "    else:\n",
    "        print(\"Feature Data: \")\n",
    "    print(\"Model name: \", newModel)\n",
    "    print(\"Epoch selected: \", epoch)\n",
    "    print(\"Batch size: \", batch_size)\n",
    "    \n",
    "def check_config_errors():\n",
    "    config_checks_output.clear_output()\n",
    "    with config_checks_output:\n",
    "        # Error checking \n",
    "        #if dataset.selected is not None:\n",
    "            #selectedFeatureDataPath = str(dataset.selected_path)\n",
    "\n",
    "        if not epoch.value.isdigit():\n",
    "            print(\"Epoch should be digit\")\n",
    "        if not batch_size.value.isdigit():\n",
    "            print(\"Batch size should be digit\")\n",
    "\n",
    "        modelPath = \"./trained_model/\"\n",
    "\n",
    "        # Display user configuration\n",
    "        modelPath += newModel.value\n",
    "        if not os.path.exists(modelPath):\n",
    "            print(\"Model \" + modelPath + \" has been initialized.\")\n",
    "            f = open(modelPath, 'w')\n",
    "            f.close()\n",
    "            print(\"Proceed to next step to train the model\")\n",
    "        else:\n",
    "            modelDir = \"./trained_model/\"\n",
    "            print(\"Model name exists or you did not specify a file name!\")     \n",
    "    \n",
    "        \n",
    "def submit_clicked(b):\n",
    "    check_config_errors()\n",
    "    \n",
    "#dynamically captures changes in input and returned new output\n",
    "training_configs = widgets.interactive_output(preview_training_config, {'newModel': newModel, 'epoch': epoch, 'batch_size': batch_size,})\n",
    "training_configs.layout = {'border': '1px solid black'}\n",
    "\n",
    "button_submit.on_click(submit_clicked)\n",
    "\n",
    "accordions = widgets.VBox([\n",
    "    accordions, \n",
    "    training_configs,\n",
    "    widgets.HBox([button_submit]),\n",
    "    config_checks_output\n",
    "])\n",
    "accordions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01df8d",
   "metadata": {},
   "source": [
    "#### US-10, US-13, US-14 & US15\n",
    "\n",
    "<font>1. Separate .py file that configures the network architecture (**run_PDAN**).</font>\n",
    "<br>\n",
    "<font>2. Set configuration to run the training sequence.</font>\n",
    "<br>\n",
    "<font>3. Display progress of the training.</font>\n",
    "<br>\n",
    "<font>4. Save newly trained models in `./trained_model`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e56b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if data path was not chosen previously\n",
    "if dataset.value == None:\n",
    "    chosen_dataset = './data/RGB_i3d_16frames_64000_SSD' \n",
    "else:\n",
    "    chosen_dataset = './data/' + dataset.value\n",
    "    \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    !chmod 755 -R ./Toyota_Smarthome/pipline/run_PDAN.sh\n",
    "    with open(\"./temp/stdout_training.txt\",\"wb\") as out, open(\"./temp/stderr.txt\",\"wb\") as err:\n",
    "        subprocess.Popen(\"./pipline/run_PDAN.sh %s %s %s %s %s %s\" % (str(chosen_dataset), str(load_model), str(\"True\"), str(epoch), str(batch_size), str(newModel)),stdout=out)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=[]\n",
    "# epoch element used for indexing epoch's training data i.e. epoch 2 loss\n",
    "epochArr = []\n",
    "trainMapArr = []\n",
    "trainLossArr = []\n",
    "valMapArr = []\n",
    "valLossArr = []\n",
    "\n",
    "with open (\"./temp/stdout_training.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        output.append(line)\n",
    "        \n",
    "for line in output:\n",
    "    if \"epoch\" in line:\n",
    "        epoch = line.replace(\"\\n\",\"\").replace(\"epoch: \",\"\").split(\" \")[1]\n",
    "        epochArr.append(int(epoch))\n",
    "    elif \"train-map\" in line:\n",
    "        trainMap = line.replace(\"train-map: \",\"\").replace(\"tensor(\",\"\").replace(\")\\n\",\"\")\n",
    "        trainMapArr.append(float(trainMap))\n",
    "    elif \"val_loss\" in line:\n",
    "        valLoss = line.replace(\"val_loss: \",\"\").replace(\"tensor(\",\"\").split(\",\")[0]\n",
    "        valLossArr.append(float(valLoss))\n",
    "    elif \"val-map\" in line:\n",
    "        valMap = line.replace(\"val-map: \",\"\").replace(\"tensor(\",\"\").replace(\")\\n\",\"\")\n",
    "        valMapArr.append(float(valMap))\n",
    "    elif \"train_loss\" in line:\n",
    "        trainLoss = line.replace(\"train_loss: \",\"\").replace(\"tensor(\",\"\").split(\",\")[0]\n",
    "        trainLossArr.append(float(trainLoss))  \n",
    "print (\"Values extracted for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade1c15",
   "metadata": {},
   "source": [
    "### TRAINING VISUALIZATION\n",
    "\n",
    "Source: https://docs.wandb.ai/guides/track/log/plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d712dd",
   "metadata": {},
   "source": [
    "#### US-29\n",
    "<font>Display the result of the training model:</font>\n",
    "1. Training Accuracy Precision\n",
    "2. Value Loss\n",
    "3. Value Accuracy Precision\n",
    "4. Training Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f751404",
   "metadata": {},
   "source": [
    "Source: https://docs.wandb.ai/quickstart\n",
    "\n",
    "<font>Login to wandb for evalutaion result evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6aa3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.login():\n",
    "    print(\"Currently logged in. Run next cell to see visualisation of training results.\")\n",
    "    print(\"If the visualisation is not displaying, request account from Whitney.\")\n",
    "else:\n",
    "    print(\"Not logged in to any account.\")\n",
    "    print(\"If API key is required, request from Whitney.\")\n",
    "\n",
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e68ed",
   "metadata": {},
   "source": [
    "<font>1. Visualize Training Accuracy Precision results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f99f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Accuracy Precision\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"3ocr5wb6\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f79fb",
   "metadata": {},
   "source": [
    "<font>2. Visualize Value Loss results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Loss\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"1dkrs5yx\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef79ff",
   "metadata": {},
   "source": [
    "<font>3. Visualize Value Accuracy Precision results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Accuracy Precision\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"296roujj\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225abdf8",
   "metadata": {},
   "source": [
    "<font>4. Visualize Training Loss results per epoch.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loss\n",
    "\n",
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"2k1f4hrp\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bca079",
   "metadata": {},
   "source": [
    "## Testing Section for TSU pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430af646",
   "metadata": {},
   "source": [
    "### R5: Evaluate a trained model based on the TSU project.\n",
    "- Choose a dataset subfolder from the data folder to use for testing\n",
    "- Load a pretrained model \n",
    "- Run the testing sequence\n",
    "- Show the progress of testing in the notebook\n",
    "- View results to allow for an assessment of how well the model performed:\n",
    "    1. Average Precision per activity class \n",
    "    2. Mean Average Precision\n",
    "- Save the results to a results folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08621e64",
   "metadata": {},
   "source": [
    "#### US-16 & US-17\n",
    "\n",
    "<font>1. Load a list of pre-trained models for testing section.</font>\n",
    "<br>\n",
    "<font>2. Select a dataset subfolder from the data folder to use for testing.</font>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a7a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing UI to get user input\n",
    "container_1 = widgets.Box()\n",
    "container_2 = widgets.Box()\n",
    "# open trained_model directory and lsit trained models there\n",
    "# selectedModel.value will be the trained model name\n",
    "\n",
    "\n",
    "trained_model_dir = './trained_model/'\n",
    "# Select a folder from the datasets folder created\n",
    "models =  os.listdir(trained_model_dir)\n",
    "\n",
    "# Select a model from model folder\n",
    "selectedModel =widgets.Select(\n",
    "    options=models,\n",
    "    # rows=10,\n",
    "    description=\"Models:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = os.listdir(data_directory)\n",
    "selectedFeatureData = widgets.Dropdown(\n",
    "    options=starting_directory,\n",
    "    value= starting_directory[0],\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "control_1 = selectedModel\n",
    "control_2 = selectedFeatureData\n",
    "container_1.children = [control_1]\n",
    "container_2.children = [control_2]\n",
    "\n",
    "inputtabs = widgets.Tab()\n",
    "inputtabs.children = [container_1, container_2]\n",
    "inputtabs.set_title(0, \"Pre-trained Model\")\n",
    "inputtabs.set_title(1, \"Feature Data\")\n",
    "inputtabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681852c",
   "metadata": {},
   "source": [
    "<font>Check your model testing configuration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad656335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check your model testing configuration.\n",
    "def inference(b):\n",
    "    print(\"Run next cell to start Testing.\")\n",
    "\n",
    "print(\"------------------------------ SUMMARY OF TESTING RUN CONFIGURATION ------------------------------\")\n",
    "print(\"Running inference with...\")\n",
    "print(\"Model: \" + selectedModel.value)\n",
    "print(\"Feature Data: \" + selectedFeatureData.value)\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Confirm Config',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Confirm Configuration',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "button.on_click(inference)\n",
    "\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8297eb25",
   "metadata": {},
   "source": [
    "#### US-18\n",
    "<font>Run testing sequences on data samples and record down the statistics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset = './data/' + selectedFeatureData.value\n",
    "chosen_model = trained_model_dir+ selectedModel.value\n",
    "\n",
    "#print(chosen_dataset, chosen_model)\n",
    "if torch.cuda.is_available():\n",
    "    !chmod 755 -R ./Toyota_Smarthome/pipline/run_PDAN.sh\n",
    "    with open(\"./temp/stdout.txt\",\"wb\") as out, open(\"./temp/stderr.txt\",\"wb\") as err:\n",
    "        subprocess.Popen(\"./pipline/run_PDAN.sh %s %s\" % (str(selectedFeatureData.selected_path), str(selectedModel.selected)),stdout=out)\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcf511",
   "metadata": {},
   "source": [
    "<font>Extract model evaluation results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "mean_AP = 0.0\n",
    "per_class_AP = []\n",
    "extractClassAP = False\n",
    "\n",
    "# read output file \n",
    "with open (\"./temp/stdout.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        output.append(line)\n",
    "\n",
    "# Extract model evaluation results\n",
    "for line in output:\n",
    "    if \"rgb MAP\" in line:\n",
    "        mean_AP = float(line.replace(\"rgb MAP: \",\"\").replace(\")\\n\",\"\").replace(\"tensor(\", \"\"))\n",
    "    elif \"rgb per vid\" in line:\n",
    "        extractClassAP = True\n",
    "        classAP = line.replace(\"rgb per vid: \",\"\").replace(\"tensor([\",\"\").replace(\"\\n\",\"\").strip().split(\",\")\n",
    "        classAP = list(filter(None, classAP))\n",
    "        for ap in classAP:\n",
    "            per_class_AP.append(float(ap))\n",
    "    elif extractClassAP:\n",
    "        classAP = line.replace(\"\\n\",\"\").replace(\"]\",\"\").replace(\")\",\"\").strip().split(\",\")\n",
    "        classAP = list(filter(None, classAP))\n",
    "        for ap in classAP:\n",
    "            per_class_AP.append(float(ap))\n",
    "    \n",
    "extractClassAP = False\n",
    "\n",
    "# Ouput the results\n",
    "print(\"Values successfully extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0f120",
   "metadata": {},
   "source": [
    "#### US-20\n",
    "<font>Display the result of the completed test:</font>\n",
    "1. Average Precision per activity class\n",
    "2. Mean Average Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18261fc2",
   "metadata": {},
   "source": [
    "Source: https://docs.wandb.ai/quickstart\n",
    "\n",
    "<font>Login to wandb for evalutaion result evaluation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1045906",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.login():\n",
    "    print(\"Currently logged in. Run next cell to see visualisation of Average Precision Per Activity Class.\")\n",
    "    print(\"If the visualisation is not displaying, request account from Whitney.\")\n",
    "else:\n",
    "    print(\"Not logged in to any account.\")\n",
    "    print(\"If API key is required, request from Whitney.\")\n",
    "\n",
    "api = wandb.Api()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567df05d",
   "metadata": {},
   "source": [
    "<font>Visualize evalutated results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1b41a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "team, project, run_id = \"whitneytwh\", \"precision-tables\", \"3pxhnhom\"\n",
    "run = api.run(f\"{team}/{project}/{run_id}\")\n",
    "\n",
    "run.display(height=720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c62f4",
   "metadata": {},
   "source": [
    "#### US-21\n",
    "<font>Save Average Precision results to a `./results` folder</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Create data folder if it does not exist \n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "#print(\"Today's date:\", today)\n",
    "\n",
    "result_directory = './results/'+str(selectedFeatureData.value)+'_'+str(selectedModel.value)+'_'+ str(today)\n",
    "\n",
    "if not os.path.exists(result_directory):\n",
    "    os.makedirs(result_directory)\n",
    "    \n",
    "print(\"Created Result folder:\" + str(result_directory))\n",
    "\n",
    "\n",
    "#resultFile = input(\"Name of File\")\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "resultFile = widgets.Text(\n",
    "    description='New Result File Name:',\n",
    "    disabled=False,\n",
    "    style=style,\n",
    ")\n",
    "\n",
    "resultFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "completeName = result_directory+'/'+resultFile.value+ \".txt\"         \n",
    "\n",
    "if not os.path.exists(completeName):\n",
    "    print(\"Created file:\" + completeName)\n",
    "    f=open(completeName, 'w')\n",
    "    f.write('average_precision = 0.3275')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e4945",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NVIDIA STEP model\n",
    "<font color='grey'>This section is for inference, training and testing using the STEP pipeline.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f12831",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inference Section for STEP pipeline\n",
    "<font>Select video input and model for inferencing. (Only P02T02C06 inferred, not enough GPU in colab to run any longer videos.)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1343e65f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set widget boxes for each variable we are loading, in this case, a pretrained model and a feature data.\n",
    "container_1 = widgets.Box()\n",
    "container_2 = widgets.Box()\n",
    "\n",
    "# open trained_model directory and lsit trained models there\n",
    "# selectedModel.value will be the trained model name\n",
    "trained_model_dir = './STEP-master/pretrained'\n",
    "models =  os.listdir(trained_model_dir)\n",
    "\n",
    "# Select a folder from the datasets folder created\n",
    "selectedModel =widgets.Select(\n",
    "    options=models,\n",
    "    # rows=10,\n",
    "    description=\"Models:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_data = './data/RGB_videos/'\n",
    "# Select a folder from the datasets folder created\n",
    "starting_directory = os.listdir(feature_data)\n",
    "selectedFeatureData = widgets.Dropdown(\n",
    "    options=starting_directory,\n",
    "    value= starting_directory[0],\n",
    "    description='Dataset:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Set variables to get selected iputs from user\n",
    "control_1 = selectedModel\n",
    "control_2 = selectedFeatureData\n",
    "container_1.children = [control_1]\n",
    "container_2.children = [control_2]\n",
    "\n",
    "# Load UI for user to select model and feature data\n",
    "inputtabs = widgets.Tab()\n",
    "inputtabs.children = [container_1, container_2]\n",
    "inputtabs.set_title(0, \"Pre-trained Model\")\n",
    "inputtabs.set_title(1, \"Feature Data\")\n",
    "inputtabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e704e2b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font>View inferencing configuration.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d6e98",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# make dir to store video frames if it doest not yet exist\n",
    "framePath = 'STEP-master/datasets/demo/frames/'\n",
    "videoNameArr = selectedFeatureData.value.split('/')\n",
    "videoName = videoNameArr[len(videoNameArr)-1].split('.')[0]\n",
    "storePath = framePath + videoName + \"/\"\n",
    "print(storePath)\n",
    "if not os.path.isdir(storePath):\n",
    "    os.mkdir(storePath)\n",
    "\n",
    "# Function to let user know what to do next after confirming configuration\n",
    "def inference(b):\n",
    "    print(\"Run next cell to start inferencing.\")\n",
    "\n",
    "\n",
    "# Output user's configuration so that user has clearer idea of what they have selected\n",
    "print(\"------------------------------ SUMMARY OF INFERENCE RUN CONFIGURATION ------------------------------\")\n",
    "print(\"Running inference with...\")\n",
    "print(\"Model: \" + str(selectedModel.value))\n",
    "print(\"Feature Data: \" + selectedFeatureData.value)\n",
    "\n",
    "# Button for user to confirm their confirmation\n",
    "button = widgets.Button(\n",
    "    description='Confirm Config',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Confirm Configuration',\n",
    "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# Display button and allow user to confirm configuration\n",
    "button.on_click(inference)\n",
    "button"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a18be7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font>Extract frames of video and store it in STEP-master for STEP pipeline input.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f96da",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get paths \n",
    "chosen_dataset = feature_data + selectedFeatureData.value\n",
    "chosen_model = trained_model_dir+selectedModel.value\n",
    "\n",
    "# Setting 16 frames per second extraction\n",
    "fps = 16\n",
    "\n",
    "# Function to get the timedelta for naming of jpgs\n",
    "def format_timedelta(td):\n",
    "    \"\"\"Utility function to format timedelta objects in a cool way (e.g 00:00:20.05) \n",
    "    omitting microseconds and retaining milliseconds\"\"\"\n",
    "    result = str(td)\n",
    "    try:\n",
    "        result, ms = result.split(\".\")\n",
    "    except ValueError:\n",
    "        return result + \".00\".replace(\":\", \"-\")\n",
    "    ms = int(ms)\n",
    "    ms = round(ms / 1e4)\n",
    "    return f\"{result}.{ms:02}\".replace(\":\", \"-\")\n",
    "\n",
    "# Function to extract the frames from the video\n",
    "def extractFrames(video_file, storePath):\n",
    "    # load the video clip\n",
    "    video_clip = VideoFileClip(video_file)\n",
    "\n",
    "    # if the SAVING_FRAMES_PER_SECOND is above video FPS, then set it to FPS (as maximum)\n",
    "    saving_frames_per_second = min(video_clip.fps, fps)\n",
    "    # if SAVING_FRAMES_PER_SECOND is set to 0, step is 1/fps, else 1/SAVING_FRAMES_PER_SECOND\n",
    "    step = 1 / video_clip.fps if saving_frames_per_second == 0 else 1 / saving_frames_per_second\n",
    "    # iterate over each possible frame\n",
    "    counter=0\n",
    "    for current_duration in np.arange(0, video_clip.duration, step):\n",
    "        # format the file name and save it\n",
    "        frame_duration_formatted = format_timedelta(timedelta(seconds=current_duration)).replace(\":\", \"-\")\n",
    "        #frame_filename = os.path.join(filename, f\"frame{frame_duration_formatted}.jpg\") /frame0000.jpg\n",
    "        frame_filename = os.path.join(storePath, f\"frame{str(counter).zfill(6)}.jpg\")\n",
    "        # save the frame with the current duration\n",
    "        counter += 1\n",
    "        video_clip.save_frame(frame_filename, current_duration)\n",
    "    print(\"Successfully extracted video frames. Please find frames in \", storePath)\n",
    "        \n",
    "extractFrames(chosen_dataset, storePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f503de",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # need specific cuda version\n",
    "    !pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    #run this first before running demo\n",
    "    !python STEP-master/setup.py build develop\n",
    "    !python STEP-master/demo.py\n",
    "else:\n",
    "    print(\"Please make sure you have NVIDIA GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19069087",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font>Merge results to output video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5dd94",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ground truth annotations\n",
    "#annotationsDir = 'Annotation/' + selectedFeature.split('.')[0][0:3] + '/' + selectedFeature.split('.')[0] + '.csv'\n",
    "'''\n",
    "# pipeline function to insert text into each prame\n",
    "def pipeline(frame):\n",
    "    try:\n",
    "        cv2.rectangle(frame, (50, 380), (250, 430), (0, 0, 0), -1)\n",
    "        cv2.rectangle(frame, (350, 380), (500, 430), (0, 0, 0), -1)\n",
    "        cv2.putText(frame, str(\"ground truth\"), (50,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, str(next(captionsDF)[1].captions), (50,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, str(\"inferred results\"), (350,400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1) \n",
    "        try:\n",
    "            cv2.putText(frame, str(next(inferredCaptions)[1].captions), (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "        except:\n",
    "            cv2.putText(frame, str(inferredDF['captions'][len(inferredDF)-1]), (350,420), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    # additional frame manipulation\n",
    "    return frame\n",
    "\n",
    "#read ground truth data\n",
    "#df = pd.read_csv(annotationsDir)\n",
    "captionsByFrame = {'captions' : []}\n",
    "caption = \"\"\n",
    "totalEvents = len(df.index)\n",
    "oldStartFrame = 0\n",
    "\n",
    "#read inferred data\n",
    "inferredDF = pd.read_csv(inferredDir)\n",
    "\n",
    "#build new df\n",
    "for event in range(totalEvents):\n",
    "    #i need to minus old one\n",
    "    initialStartframe = int(df['start_frame'][event]) - oldStartFrame\n",
    "    for n in range(initialStartframe):\n",
    "        captionsByFrame['captions'].append(caption)\n",
    "    caption = str(df['event'][event])\n",
    "    oldStartFrame += initialStartframe\n",
    "    if (event == totalEvents-1):\n",
    "        initialStartframe = int(df['end_frame'][event]) - oldStartFrame\n",
    "        for n in range(initialStartframe):\n",
    "            captionsByFrame['captions'].append(caption)\n",
    "\n",
    "    \n",
    "captionsDF = pd.DataFrame(captionsByFrame).iterrows()\n",
    "inferredCaptions = inferredDF.iterrows()\n",
    "video = VideoFileClip(videoSelectedPath)\n",
    "out_video = video.fl_image(pipeline)\n",
    "\n",
    "# output path\n",
    "captionVideoPath = './video_withcaptions/' + selectedVideo.selected_filename\n",
    "out_video.write_videofile(captionVideoPath, audio=True)\n",
    "'''\n",
    "import moviepy\n",
    "# Set path to read inferred results\n",
    "videoName = selectedFeatureData.value.split('.')[0]\n",
    "resultPath = 'STEP-master/datasets/demo/frames/results/'\n",
    "outputPath = resultPath + videoName + '/*.jpg'\n",
    "\n",
    "# Merge all ouputted images and output a video\n",
    "img_array = []\n",
    "for filename in glob.glob(outputPath):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width, height)\n",
    "    img_array.append(img)\n",
    "\n",
    "outPath = 'STEP-master/datasets/demo/results/'\n",
    "resultName = outPath + videoName + '.mp4'\n",
    "    \n",
    "# make output dir if it does not exist\n",
    "if not os.path.isdir(outPath):\n",
    "    os.mkdir(outPath)\n",
    "    \n",
    "\n",
    "image_files = [os.path.join(resultPath+videoName, img)\n",
    "              for img in os.listdir(resultPath+videoName)\n",
    "              if img.endswith(\".jpg\")]\n",
    "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=16)\n",
    "clip.write_videofile('STEP-master/datasets/demo/results/test.mp4')\n",
    "\n",
    "print(\"Successfully merged video!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda8a56",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font>Play the video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ead07f",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#resultName = './' + resultName\n",
    "print(resultName)\n",
    "Video('STEP-master/datasets/demo/results/test.mp4', embed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767651d5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training Section for STEP pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb211e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font>Set up and infer using STEP pipeline. (Need GPU)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd63bf6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font> (Need GPU)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c78817",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert csv to pkl for training and testing\n",
    "!python STEP-master/scripts/generate_label.py \"STEP-master/datasets/generate_label/ava_train_v2.2.csv\"\n",
    "!python STEP-master/scripts/generate_label.py \"STEP-master/datasets/generate_label/ava_val_v2.2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ebf83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate frames for testing\n",
    "!python STEP-master/scripts/extract_clips.py"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
